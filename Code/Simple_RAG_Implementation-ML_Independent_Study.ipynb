{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70577418",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Simple RAG Implementation\n",
    "Author: Christian Sarmiento\n",
    "Purpose: This notebook is intended to get a simple implementation of RAG set up with LangChain.\n",
    "Date Created: 10/1/24\n",
    "Last Updated: 11/28/24\n",
    "Data: https://archive.ics.uci.edu/dataset/450/sports+articles+for+objectivity+analysis\n",
    "Sources:\n",
    "- https://python.langchain.com/docs/tutorials/rag/\n",
    "- https://python.langchain.com/docs/tutorials/llm_chain/\n",
    "- https://medium.com/@dinabavli/rag-basics-basic-implementation-of-retrieval-augmented-generation-rag-e80e0791159d\n",
    "- ChatGPT: o1-preview\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "RAG Research             |               Machine Learning Independent Study             |              DR. EITEL LAURIA\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "db84d629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ragas in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (0.2.3)\n",
      "Requirement already satisfied: numpy in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from ragas) (1.26.4)\n",
      "Requirement already satisfied: datasets in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from ragas) (3.1.0)\n",
      "Requirement already satisfied: tiktoken in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from ragas) (0.7.0)\n",
      "Requirement already satisfied: langchain in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from ragas) (0.3.1)\n",
      "Requirement already satisfied: langchain-core in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from ragas) (0.3.6)\n",
      "Requirement already satisfied: langchain-community in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from ragas) (0.3.1)\n",
      "Requirement already satisfied: langchain-openai in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from ragas) (0.2.1)\n",
      "Requirement already satisfied: nest-asyncio in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from ragas) (1.6.0)\n",
      "Requirement already satisfied: appdirs in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from ragas) (1.4.4)\n",
      "Requirement already satisfied: pydantic>=2 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from ragas) (2.9.2)\n",
      "Requirement already satisfied: openai>1 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from ragas) (1.50.0)\n",
      "Requirement already satisfied: pysbd>=0.3.4 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from ragas) (0.3.4)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from openai>1->ragas) (4.6.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from openai>1->ragas) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from openai>1->ragas) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from openai>1->ragas) (0.5.0)\n",
      "Requirement already satisfied: sniffio in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from openai>1->ragas) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from openai>1->ragas) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from openai>1->ragas) (4.11.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from pydantic>=2->ragas) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from pydantic>=2->ragas) (2.23.4)\n",
      "Requirement already satisfied: filelock in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from datasets->ragas) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from datasets->ragas) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from datasets->ragas) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from datasets->ragas) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from datasets->ragas) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from datasets->ragas) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from datasets->ragas) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets->ragas) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from datasets->ragas) (3.10.6)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from datasets->ragas) (0.25.1)\n",
      "Requirement already satisfied: packaging in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from datasets->ragas) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from datasets->ragas) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from langchain->ragas) (2.0.35)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from langchain->ragas) (0.3.0)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from langchain->ragas) (0.1.129)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from langchain->ragas) (8.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from langchain-core->ragas) (1.33)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from langchain-community->ragas) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from langchain-community->ragas) (2.5.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from tiktoken->ragas) (2024.9.11)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from aiohttp->datasets->ragas) (2.4.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from aiohttp->datasets->ragas) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from aiohttp->datasets->ragas) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from aiohttp->datasets->ragas) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from aiohttp->datasets->ragas) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from aiohttp->datasets->ragas) (1.13.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai>1->ragas) (3.10)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (3.22.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (0.9.0)\n",
      "Requirement already satisfied: certifi in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai>1->ragas) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai>1->ragas) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>1->ragas) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core->ragas) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain->ragas) (3.10.7)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community->ragas) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from requests>=2.32.2->datasets->ragas) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from requests>=2.32.2->datasets->ragas) (2.2.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain->ragas) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from pandas->datasets->ragas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from pandas->datasets->ragas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from pandas->datasets->ragas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets->ragas) (1.16.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: nltk in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from nltk) (4.66.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Download RAGAS for RAG metrics\n",
    "%pip install ragas\n",
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f6dfada",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in _VertexAIBase has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in _VertexAICommon has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import sys\n",
    "sys.path.append(\"/Users/christiansarmiento/Library/CloudStorage/OneDrive-MaristCollege/Machine Learning/Private Code\")\n",
    "from api_keys import openAIKey\n",
    "from api_keys import langchainKey\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain import hub  # for RAG prompt\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import pandas as pd\n",
    "import os\n",
    "import gradio as gr  # easy frontend implementation\n",
    "import numpy as np\n",
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, SemanticSimilarity\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas import SingleTurnSample, EvaluationDataset\n",
    "from ragas import evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6cc71a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain Enviornment Variables\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = langchainKey()\n",
    "os.environ[\"OPENAI_API_KEY\"] = openAIKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99387228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OpenAI model with metric wrapper\n",
    "#llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
    "#evalEmbeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc182a12-6a70-4fb9-8f77-823e341f989a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "folderPath = \"/Users/christiansarmiento/Library/CloudStorage/OneDrive-MaristCollege/Machine Learning/Data/sports_articles_corpus/Raw data\"\n",
    "sportsArticles = []\n",
    "\n",
    "for fileName in os.listdir(folderPath):\n",
    "    filePath = os.path.join(folderPath, fileName)\n",
    "    loader = TextLoader(filePath, encoding='latin1')  # UTF-8 not working for the files\n",
    "    doc = loader.load()\n",
    "    sportsArticles.extend(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c567c173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Documents into Chunks\n",
    "textSplitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, add_start_index=True)\n",
    "texts = textSplitter.split_documents(sportsArticles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2646002f-c802-441e-bd43-adb3bb76c2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store Documents in Vector DB (Chroma)\n",
    "vectorDB = Chroma.from_documents(documents=texts, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac4c8790-f634-40b7-9d96-841c65f5f9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Retrieval System\n",
    "retriever = vectorDB.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})  # Retrieves 3 documents\n",
    "\n",
    "# To get retrieved documents:\n",
    "# retrievedDocuments = retriever.invoke(\"query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f6b342-08b9-4e63-b946-280e3590ac85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the RAG Chain\n",
    "\n",
    "# Function to format documents into the prompt\n",
    "def formatDocs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Setup RAG Chain\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "ragChain = (\n",
    "    {\"context\": retriever | formatDocs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24e7ebd5-e639-491d-aaaf-636cd788b166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The offside rule in soccer states that a player is in an offside position if they are nearer to the opponent's goal line than both the ball and the second-to-last opponent when the ball is played to them, unless they are in their own half or level with the second-to-last opponent. Being in an offside position is not an offense in itself; the player must become involved in active play to be penalized. The rule aims to prevent players from gaining an unfair advantage by lingering near the opponent's goal."
     ]
    }
   ],
   "source": [
    "# Results\n",
    "for chunk in ragChain.stream(\"Explain the offside rule in soccer.\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f88a596-3c50-43af-9ca5-bcb13feb4570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if answers are coming from the llm or from the documents\n",
    "# Try giving documents that aren't real then asking questions on things off of that\n",
    "# Avoids the model relying on trained info \n",
    "# Play with the system prompt\n",
    "\n",
    "# Next step after QA - feed answers into the system to make it more conversational\n",
    "# Implement Gradio\n",
    "# Knowledge Graph \n",
    "# Identifying Metrics - do research!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d5d2da-5ee6-4dd7-8008-8ee8a751d297",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Implement with Marist Data\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49d2eaa0-434f-409a-8299-d94c24d4f207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "csvPath = \"/Users/christiansarmiento/Library/CloudStorage/OneDrive-MaristCollege/Machine Learning/Data/Marist_QA.csv\"\n",
    "maristQA = pd.read_csv(csvPath, header=None)\n",
    "\n",
    "# To use RecursiveCharacterTextSplitter, we need a list of dictionaries\n",
    "maristContext = [Document(page_content=text) for text in maristQA[1].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e06dbe67-e698-4083-8998-370ff78fc21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Documents into Chunks\n",
    "textSplitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, add_start_index=True)\n",
    "texts = textSplitter.split_documents(maristContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dac6137-0ecf-4c22-9c6a-ec8c86c6a134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store Documents in Vector DB (Chroma)\n",
    "vectorDB = Chroma.from_documents(documents=texts, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cfc0f3a-3933-4dd7-8558-d9b240a83e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Retrieval System\n",
    "retriever = vectorDB.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})  # Retrieves 3 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd26cb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts\n",
    "systemPrompt = (\n",
    "    \n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    "    \n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    \n",
    "    [\n",
    "        (\"system\", systemPrompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    "    \n",
    ")\n",
    "\n",
    "contextualizeSystemPrompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualizePrompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualizeSystemPrompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "historyAwareRetriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualizePrompt\n",
    ")\n",
    "\n",
    "qaPrompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", systemPrompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dad046b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make chains\n",
    "questionAnswerChain = create_stuff_documents_chain(llm, qaPrompt)\n",
    "ragChain = create_retrieval_chain(historyAwareRetriever, questionAnswerChain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e91b900d-6610-46a6-be4b-4c76a99e70f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marist College is located on the banks of the Hudson River and also has a campus in Florence, Italy.\n",
      "\n",
      "Marist College is in Poughkeepsie, New York, situated along the Hudson River. The Florence campus is located in Florence, Italy.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Talking to ChatGPT, making RAG conversational\n",
    "conversationHistory = []\n",
    "userQuery = input(\"Prompt (0 to quit): \")\n",
    "while userQuery != '0':\n",
    "\n",
    "    # Print input - this is just for a VSCode enviornment to see I/O together, feel free to comment out in Jupyter\n",
    "    print(f\"User: {userQuery}\")\n",
    "\n",
    "    # Call ChatGPT using RAG chain\n",
    "    llmResponse = ragChain.invoke({\"input\": userQuery, \"chat_history\": conversationHistory})\n",
    "    print(f\"LLM: {llmResponse['answer']}\")\n",
    "    print()\n",
    "    conversationHistory.extend([\n",
    "        \n",
    "        HumanMessage(content=userQuery),\n",
    "        AIMessage(content=llmResponse[\"answer\"]),\n",
    "        \n",
    "    ])\n",
    "\n",
    "    # New prompt\n",
    "    userQuery = input(\"Prompt (0 to quit): \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39c79824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frontend with Gradio\n",
    "\n",
    "'''\n",
    "Function that calls the RAG chain for Gradio\n",
    "'''\n",
    "evaluationSamples = []\n",
    "def simpleRAG(userQuery, history, correctAnswer):\n",
    "\n",
    "    # Ensure there is a list to use for the conversation history\n",
    "    if history is None:\n",
    "        history = []\n",
    "\n",
    "    # Call ChatGPT using RAG chain\n",
    "    llmResponse = ragChain.invoke({\"input\": userQuery, \"chat_history\": history})\n",
    "\n",
    "    # Get response and context for evaluation\n",
    "    responseText = llmResponse[\"answer\"]\n",
    "    retrievedContexts = [context.page_content for context in retriever.get_relevant_documents(userQuery)]\n",
    "\n",
    "    # Save information for RAG metrics\n",
    "    evaluationSamples.append({\n",
    "        \"user_input\": userQuery,\n",
    "        \"retrieved_contexts\": retrievedContexts,\n",
    "        \"response\": responseText,\n",
    "        \"reference\": correctAnswer    # ground truth \n",
    "    })\n",
    "\n",
    "    # Save chat history for conversational aspect\n",
    "    history.extend([\n",
    "        \n",
    "        HumanMessage(content=userQuery),\n",
    "        AIMessage(content=llmResponse[\"answer\"]),\n",
    "        \n",
    "    ])\n",
    "\n",
    "    # Save input and output to history\n",
    "    history.append(HumanMessage(content=userQuery))\n",
    "    history.append(AIMessage(content=llmResponse[\"answer\"]))\n",
    "\n",
    "    # Prepare display of data\n",
    "    #chatDisplay = [(msg.content, \"User\" if isinstance(msg, HumanMessage) else \"LLM\") for msg in history]\n",
    "\n",
    "    return history  #, chatDisplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9c3853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frontend\n",
    "interface = gr.Interface(\n",
    "    fn=simpleRAG,  \n",
    "    inputs=[\"text\", \"state\", gr.Textbox(label=\"Correct Answer\")],  \n",
    "    outputs=[\"chatbot\", \"state\"],  \n",
    "    title=\"Simple RAG\",  \n",
    "    description=\"Initial setup for a simple conversational RAG process.\"\n",
    ")\n",
    "\n",
    "# Launch the frontend\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e13b080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate our RAG pipeline\n",
    "async def pipelineEvaluation(dataset, metrics):\n",
    "\n",
    "    # Run through our runs\n",
    "    results = []\n",
    "    for run in dataset:\n",
    "\n",
    "        # Save our inputs/outputs\n",
    "        inputQuery = run[\"user_input\"]\n",
    "        groundTruthAnswer = run[\"reference\"]\n",
    "        contexts = run[\"retrieved_contexts\"]\n",
    "        response = run[\"response\"]\n",
    "\n",
    "        # Create a SingleTurnSample object\n",
    "        sample = SingleTurnSample(\n",
    "            user_input=inputQuery,\n",
    "            response=response,\n",
    "            reference=groundTruthAnswer,\n",
    "            retrieved_contexts=contexts \n",
    "        )\n",
    "\n",
    "        # Evaluate metrics\n",
    "        runResults = {\"input_query\": inputQuery}\n",
    "        for metric in metrics:\n",
    "\n",
    "            # Get the score for the given metric\n",
    "            try:\n",
    "\n",
    "                score = await metric.single_turn_ascore(sample)\n",
    "                runResults[type(metric).__name__] = score\n",
    "\n",
    "            except Exception as e:\n",
    "                # Catch errors for debugging\n",
    "                runResults[type(metric).__name__] = f\"Error: {str(e)}\"\n",
    "        \n",
    "        # Save metric results\n",
    "        results.append(runResults)\n",
    "\n",
    "    # Calculate mean and standard deviation for each metric\n",
    "    metricsStats = {}\n",
    "    for metric in metrics:\n",
    "        metricName = type(metric).__name__\n",
    "        scores = [result[metricName] for result in results if isinstance(result[metricName], (int, float))]\n",
    "        \n",
    "        # Only calculate stats if there are valid scores\n",
    "        if scores:\n",
    "            metricsStats[metricName] = {\n",
    "                \"mean\": np.mean(scores),\n",
    "                \"std_dev\": np.std(scores),\n",
    "            }\n",
    "            \n",
    "        else:\n",
    "            metricsStats[metricName] = {\n",
    "                \"mean\": \"No valid scores\",\n",
    "                \"std_dev\": \"No valid scores\",\n",
    "            }\n",
    "    \n",
    "    return results, metricsStats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce47ce73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metrics\n",
    "evalMetrics = [LLMContextRecall(llm=LangchainLLMWrapper(llm)), \n",
    "               FactualCorrectness(llm=LangchainLLMWrapper(llm)), \n",
    "               Faithfulness(llm=LangchainLLMWrapper(llm)), \n",
    "               SemanticSimilarity(embeddings=LangchainEmbeddingsWrapper(OpenAIEmbeddings()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ee9978b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_query': 'Who is Carolyn Matheus?', 'LLMContextRecall': 0.0, 'FactualCorrectness': 0.0, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.9328844088386496}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate our pipeline responses\n",
    "evalResults = await pipelineEvaluation(evaluationSamples, evalMetrics)\n",
    "for result in evalResults:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2118e23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>Can the college help me plan for graduate school?</td>\n",
       "      <td>\"Center for Career ServicesYour Path to Succes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>When was the tunnel made?</td>\n",
       "      <td>\"About Marist CollegeMarist History: 2008-Pres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>When are computer science classes held?</td>\n",
       "      <td>\"Master of Science in Computer Science \\u2013 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>How many people are on the presidential search...</td>\n",
       "      <td>\"Presidential SearchApril 15, 2021 Memo to the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>Where is student financial services?</td>\n",
       "      <td>\"Transfer Student Admission Department Student...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0  \\\n",
       "147  Can the college help me plan for graduate school?   \n",
       "136                          When was the tunnel made?   \n",
       "362            When are computer science classes held?   \n",
       "192  How many people are on the presidential search...   \n",
       "588               Where is student financial services?   \n",
       "\n",
       "                                                     1  \n",
       "147  \"Center for Career ServicesYour Path to Succes...  \n",
       "136  \"About Marist CollegeMarist History: 2008-Pres...  \n",
       "362  \"Master of Science in Computer Science \\u2013 ...  \n",
       "192  \"Presidential SearchApril 15, 2021 Memo to the...  \n",
       "588  \"Transfer Student Admission Department Student...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample 222 records from our dataset\n",
    "maristTestSample = maristQA.sample(40, replace=False)\n",
    "maristTestSample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae9bb074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_query': 'Can visitors park on campus?', 'LLMContextRecall': 1.0, 'FactualCorrectness': 0.12, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.8174370158601927}\n",
      "{'input_query': 'What is the cost of ASIP Program?', 'LLMContextRecall': 0.8, 'FactualCorrectness': 0.19, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.8295583480298927}\n",
      "{'input_query': \"Wat is Dr. Wermuth's phone number?\", 'LLMContextRecall': 0.2, 'FactualCorrectness': 0.0, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.7584736795890439}\n",
      "{'input_query': 'Who is Desiree Dighton?', 'LLMContextRecall': 1.0, 'FactualCorrectness': 0.41, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.9484875438147378}\n",
      "{'input_query': 'How many courses should be applied by a student in  global studies.', 'LLMContextRecall': 0.5, 'FactualCorrectness': 0.11, 'Faithfulness': 0.6666666666666666, 'SemanticSimilarity': 0.8698464308443776}\n",
      "{'input_query': 'What are some places students have been able to intern?', 'LLMContextRecall': 0.6, 'FactualCorrectness': 0.08, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.8036229480667897}\n",
      "{'input_query': 'What about Blackridge technology', 'LLMContextRecall': 0.25, 'FactualCorrectness': 0.0, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.7057238016566779}\n",
      "{'input_query': 'email of Dr. Wermuth?', 'LLMContextRecall': 0.25, 'FactualCorrectness': 0.0, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.7762219347936512}\n",
      "{'input_query': 'Contact details for Graduate admissions?', 'LLMContextRecall': 0.16666666666666666, 'FactualCorrectness': 0.18, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.8980322639687551}\n",
      "{'input_query': 'When are flu shots due by?', 'LLMContextRecall': 0.5, 'FactualCorrectness': 0.11, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.7993045143857638}\n",
      "{'input_query': 'Where do I get help with tuition?', 'LLMContextRecall': 0.7777777777777778, 'FactualCorrectness': 0.3, 'Faithfulness': 0.75, 'SemanticSimilarity': 0.9133611312145262}\n",
      "{'input_query': 'What does Dr. Kate Chaterdon teach?', 'LLMContextRecall': 0.7777777777777778, 'FactualCorrectness': 0.15, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.926256268263924}\n",
      "{'input_query': 'Role of lisa neilson?', 'LLMContextRecall': 0.42857142857142855, 'FactualCorrectness': 0.24, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.9458991322082343}\n",
      "{'input_query': 'are classes at marist big?', 'LLMContextRecall': 0.1, 'FactualCorrectness': 0.45, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.8753513254291918}\n",
      "{'input_query': 'Job of Sasha L. Biro?', 'LLMContextRecall': 0.625, 'FactualCorrectness': 0.18, 'Faithfulness': 0.6666666666666666, 'SemanticSimilarity': 0.911312251180301}\n",
      "{'input_query': 'Interactive media department info', 'LLMContextRecall': 1.0, 'FactualCorrectness': 0.51, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.9452108588396261}\n",
      "{'input_query': 'Janine Larmon Peterson', 'LLMContextRecall': 0.3, 'FactualCorrectness': 0.35, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.9553079750575204}\n",
      "{'input_query': 'Can I take online classes at Marist?', 'LLMContextRecall': 0.5, 'FactualCorrectness': 0.24, 'Faithfulness': 0.5, 'SemanticSimilarity': 0.8507167708502208}\n",
      "{'input_query': 'What is Dr. Wermuths phone nubmer?', 'LLMContextRecall': 0.0, 'FactualCorrectness': 0.0, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.7584736795890439}\n",
      "{'input_query': ' Where did Dr. Cathleen Muller go to school?', 'LLMContextRecall': 0.75, 'FactualCorrectness': 0.0, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.8796603921254}\n",
      "{'input_query': \"What are Frieburghus's areas of interest?\", 'LLMContextRecall': 0.0, 'FactualCorrectness': 0.17, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.9065134236809378}\n",
      "{'input_query': \"What does the Registrar's office do?\", 'LLMContextRecall': 0.42105263157894735, 'FactualCorrectness': 0.32, 'Faithfulness': 0.625, 'SemanticSimilarity': 0.9130239225243042}\n",
      "{'input_query': 'Graduate Study in the UK', 'LLMContextRecall': 0.6363636363636364, 'FactualCorrectness': 0.21, 'Faithfulness': 0.1111111111111111, 'SemanticSimilarity': 0.690506836997538}\n",
      "{'input_query': 'Can meal swipes be used for anything else other than food?', 'LLMContextRecall': 0.0, 'FactualCorrectness': 0.18, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.7580689786443069}\n",
      "{'input_query': 'What career services are available to me as a student?', 'LLMContextRecall': 0.7692307692307693, 'FactualCorrectness': 0.4, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.9240229769155403}\n",
      "{'input_query': 'Who is Richard Lewis', 'LLMContextRecall': 0.0, 'FactualCorrectness': 0.37, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.9524615761745528}\n",
      "{'input_query': 'about Dr. Schrier', 'LLMContextRecall': 0.9, 'FactualCorrectness': 0.44, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.9600576364056206}\n",
      "{'input_query': 'What courses does Shannon Roper teach?', 'LLMContextRecall': 0.75, 'FactualCorrectness': 0.73, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.9137527036400217}\n",
      "{'input_query': 'Is there a strategic committee at the school?', 'LLMContextRecall': 0.4, 'FactualCorrectness': 0.25, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.885801609722892}\n",
      "{'input_query': 'What is the Marist partnership with IBM?', 'LLMContextRecall': 0.3125, 'FactualCorrectness': 0.48, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.9483471249505935}\n",
      "{'input_query': 'Who is considered an adult undergraduate student?', 'LLMContextRecall': 0.5, 'FactualCorrectness': 0.0, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.8544509326061333}\n",
      "{'input_query': 'Is business analytics a lot of coding?', 'LLMContextRecall': 0.07692307692307693, 'FactualCorrectness': 0.0, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.7990851527322766}\n",
      "{'input_query': 'Tell me about Stephen M. Mercier', 'LLMContextRecall': 0.7333333333333333, 'FactualCorrectness': 0.0, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.8475053361226084}\n",
      "{'input_query': 'What are the Internship opportunities for  school of Computer Science and Mathematics?', 'LLMContextRecall': 0.5, 'FactualCorrectness': 0.25, 'Faithfulness': 0.875, 'SemanticSimilarity': 0.8384430149042045}\n",
      "{'input_query': 'Do I have to research something specific for my honors thesis?', 'LLMContextRecall': 0.75, 'FactualCorrectness': 0.12, 'Faithfulness': 0.75, 'SemanticSimilarity': 0.7981737434102689}\n",
      "{'input_query': 'How to contact Safety and security?', 'LLMContextRecall': 0.5, 'FactualCorrectness': 0.05, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.921631351945191}\n",
      "{'input_query': 'Who is Dr. Kate Chaterdon?', 'LLMContextRecall': 0.9090909090909091, 'FactualCorrectness': 0.32, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.9557422003351805}\n",
      "{'input_query': 'Is professor Butsko an advisor?', 'LLMContextRecall': 0.0, 'FactualCorrectness': 0.16, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.8952921159930588}\n",
      "{'input_query': 'Who is the president of Marist College?', 'LLMContextRecall': 0.0, 'FactualCorrectness': 0.11, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.9074312335119354}\n",
      "{'input_query': 'WHere did Zachary Arth graduate from?', 'LLMContextRecall': 0.0, 'FactualCorrectness': 0.0, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.8712820273608326}\n",
      "{'input_query': 'What subjects are part of the Upward Bound curriculum? ', 'LLMContextRecall': 0.4, 'FactualCorrectness': 0.28, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.870901399295597}\n",
      "{'input_query': 'Who is Mark James Morreal?', 'LLMContextRecall': 0.25, 'FactualCorrectness': 0.42, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.9225712511933144}\n",
      "{'input_query': 'When will I get marist alerts?', 'LLMContextRecall': 0.7346938775510204, 'FactualCorrectness': 0.28, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.9211349205389218}\n",
      "{'input_query': 'where do I go to get help with my resume?', 'LLMContextRecall': 0.8, 'FactualCorrectness': 0.14, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.8764942309676491}\n",
      "{'input_query': 'When is the the office of financial services open?', 'LLMContextRecall': 1.0, 'FactualCorrectness': 0.0, 'Faithfulness': 0.7142857142857143, 'SemanticSimilarity': 0.7869744610986678}\n",
      "{'input_query': 'What is the minimum gpa to get an internship?', 'LLMContextRecall': 0.16666666666666666, 'FactualCorrectness': 0.15, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.8060657492303884}\n",
      "{'input_query': 'Having questions about your Marist application?', 'LLMContextRecall': 1.0, 'FactualCorrectness': 0.38, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.9424479324456229}\n",
      "{'input_query': 'Department of Art and Digital media chair', 'LLMContextRecall': 0.6470588235294118, 'FactualCorrectness': 0.16, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.9262893510928931}\n",
      "{'input_query': 'What is FFE?', 'LLMContextRecall': 1.0, 'FactualCorrectness': 0.5, 'Faithfulness': 0.625, 'SemanticSimilarity': 0.9377608870858414}\n",
      "{'input_query': 'Are DMA members eligible for special tuition pricing?', 'LLMContextRecall': 0.3333333333333333, 'FactualCorrectness': 0.22, 'Faithfulness': 0.75, 'SemanticSimilarity': 0.8754755813703375}\n",
      "{'input_query': 'Deadline for selecting pathway?\\n', 'LLMContextRecall': 0.8571428571428571, 'FactualCorrectness': 0.08, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.8315044493759463}\n",
      "{'input_query': 'Dr. Gregory Machacek', 'LLMContextRecall': 0.0, 'FactualCorrectness': 0.7, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.9378316549757473}\n",
      "{'input_query': 'Who is John Allan Knight?', 'LLMContextRecall': 0.0, 'FactualCorrectness': 0.1, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.8714031464448483}\n",
      "{'input_query': 'Who is the director of communications?', 'LLMContextRecall': 0.75, 'FactualCorrectness': 0.0, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.8530419016305933}\n",
      "{'input_query': \"Why is a red fox the college's mascot?\", 'LLMContextRecall': 0.3333333333333333, 'FactualCorrectness': 0.06, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.8518044294883672}\n",
      "{'input_query': 'MSIS concentrations', 'LLMContextRecall': 0.125, 'FactualCorrectness': 0.0, 'Faithfulness': 0.4, 'SemanticSimilarity': 0.8721568556404287}\n",
      "{'input_query': 'Where do most activities occur on campus?', 'LLMContextRecall': 0.1111111111111111, 'FactualCorrectness': 0.29, 'Faithfulness': 0.5555555555555556, 'SemanticSimilarity': 0.911491764345361}\n",
      "{'input_query': 'Jodi Hartmann information', 'LLMContextRecall': 1.0, 'FactualCorrectness': 0.21, 'Faithfulness': 0.2222222222222222, 'SemanticSimilarity': 0.8947498026131}\n",
      "{'input_query': 'Dr. Kate Chaterdon bio?', 'LLMContextRecall': 0.0, 'FactualCorrectness': 0.0, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.828543963822664}\n",
      "{'input_query': 'Who is the chair of the history department?', 'LLMContextRecall': 0.25, 'FactualCorrectness': 0.0, 'Faithfulness': 0.4, 'SemanticSimilarity': 0.8523411165924177}\n",
      "{'input_query': 'What are MBA courses like?', 'LLMContextRecall': 0.8125, 'FactualCorrectness': 0.06, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.8954707472524589}\n",
      "{'input_query': 'Dr. Arth research', 'LLMContextRecall': 1.0, 'FactualCorrectness': 0.57, 'Faithfulness': 0.8888888888888888, 'SemanticSimilarity': 0.9447343177466511}\n",
      "{'input_query': 'Annamaria Maciocia info', 'LLMContextRecall': 1.0, 'FactualCorrectness': 0.6, 'Faithfulness': 0.5, 'SemanticSimilarity': 0.9371915127832885}\n",
      "{'input_query': 'Where are the emergency phones located at Marist?', 'LLMContextRecall': 0.5454545454545454, 'FactualCorrectness': 0.09, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.9169484145607283}\n",
      "{'input_query': 'What sports is Malick Ndiaye interested in?', 'LLMContextRecall': 1.0, 'FactualCorrectness': 0.4, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.8170100517491894}\n",
      "{'input_query': 'Dr. Kate Chaterdon bio?', 'LLMContextRecall': 0.6, 'FactualCorrectness': 0.41, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.9540173670648071}\n",
      "{'input_query': 'Who is Irma Blanco Casey?', 'LLMContextRecall': 0.0, 'FactualCorrectness': 0.0, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.9221643006984115}\n",
      "{'input_query': 'Pre-college program price', 'LLMContextRecall': 0.3333333333333333, 'FactualCorrectness': 0.0, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.8587910450168119}\n",
      "{'input_query': 'What makes Marist College special?', 'LLMContextRecall': 0.75, 'FactualCorrectness': 0.27, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.9039938547161672}\n",
      "{'input_query': \"When and where are Upward Bound's services provided?\", 'LLMContextRecall': 0.375, 'FactualCorrectness': 0.24, 'Faithfulness': 0.4444444444444444, 'SemanticSimilarity': 0.9289488613205434}\n",
      "{'input_query': 'descrive Maureen Melita', 'LLMContextRecall': 0.9, 'FactualCorrectness': 0.47, 'Faithfulness': 0.9166666666666666, 'SemanticSimilarity': 0.935869385032188}\n",
      "{'input_query': 'tell me about the Proofreading program', 'LLMContextRecall': 0.0, 'FactualCorrectness': 0.06, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.8422190821664812}\n",
      "{'input_query': 'What is the point of the core curriculum?', 'LLMContextRecall': 0.5555555555555556, 'FactualCorrectness': 0.43, 'Faithfulness': 0.6666666666666666, 'SemanticSimilarity': 0.9265849858270732}\n",
      "{'input_query': 'What is HSF?\\n', 'LLMContextRecall': 0.0, 'FactualCorrectness': 0.19, 'Faithfulness': 0.4, 'SemanticSimilarity': 0.8252781215862522}\n",
      "{'input_query': 'How do I make an appointment with the Writing Center?', 'LLMContextRecall': 0.5714285714285714, 'FactualCorrectness': 0.22, 'Faithfulness': 0.75, 'SemanticSimilarity': 0.9181455312405602}\n",
      "{'input_query': 'Where is commuter parking located?', 'LLMContextRecall': 0.9230769230769231, 'FactualCorrectness': 0.07, 'Faithfulness': 0.3333333333333333, 'SemanticSimilarity': 0.8854400890935465}\n",
      "{'input_query': 'School housing for graduate students?', 'LLMContextRecall': 0.3333333333333333, 'FactualCorrectness': 0.22, 'Faithfulness': 0.75, 'SemanticSimilarity': 0.8712617508845502}\n",
      "{'input_query': 'Physician salary', 'LLMContextRecall': 0.8333333333333334, 'FactualCorrectness': 0.0, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.766553192484862}\n",
      "{'input_query': 'How long is MA in mental health?', 'LLMContextRecall': 0.6666666666666666, 'FactualCorrectness': 0.25, 'Faithfulness': 0.8, 'SemanticSimilarity': 0.9240934815430188}\n",
      "{'input_query': 'What does Prodessor Scepanski teach?', 'LLMContextRecall': 1.0, 'FactualCorrectness': 0.0, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.8495595805951464}\n",
      "{'input_query': 'What does the Office of International Student Services do?', 'LLMContextRecall': 0.5, 'FactualCorrectness': 0.61, 'Faithfulness': 0.6666666666666666, 'SemanticSimilarity': 0.95782687285518}\n",
      "{'input_query': 'What are the scholarship benefits for transfer students from Dutchess Community College?', 'LLMContextRecall': 0.7142857142857143, 'FactualCorrectness': 0.33, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.8638953819654889}\n",
      "{'input_query': 'Mark James Morreale history?', 'LLMContextRecall': 0.375, 'FactualCorrectness': 0.5, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.9427676076342025}\n",
      "{'input_query': 'Are flu shots required for students?', 'LLMContextRecall': 0.6, 'FactualCorrectness': 0.21, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.8363879555916599}\n",
      "{'input_query': 'What is title 9?', 'LLMContextRecall': 0.5, 'FactualCorrectness': 0.4, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.9265263234828882}\n",
      "{'input_query': 'How can I learn more about being a student at Marist?', 'LLMContextRecall': 0.2222222222222222, 'FactualCorrectness': 0.21, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.8979702540568295}\n",
      "{'input_query': 'How do you pick a new president?', 'LLMContextRecall': 0.8888888888888888, 'FactualCorrectness': 0.26, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.9019868295257226}\n",
      "{'input_query': 'How do I get marist alerts?', 'LLMContextRecall': 0.7872340425531915, 'FactualCorrectness': 0.29, 'Faithfulness': 0.25, 'SemanticSimilarity': 0.9468006871379568}\n",
      "{'input_query': 'Who teaches The Interview?', 'LLMContextRecall': 0.6, 'FactualCorrectness': 0.0, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.8474309026794076}\n",
      "{'input_query': 'Professional Accountancy graduate earnings', 'LLMContextRecall': 0.6, 'FactualCorrectness': 0.41, 'Faithfulness': 0.5, 'SemanticSimilarity': 0.8827290746031828}\n",
      "{'input_query': 'Where did Dr. Schrier graduate from?', 'LLMContextRecall': 0.0, 'FactualCorrectness': 0.0, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.8478955939158798}\n",
      "{'input_query': 'Dr. Eden cyber bullying research', 'LLMContextRecall': 0.6666666666666666, 'FactualCorrectness': 0.24, 'Faithfulness': 0.8571428571428571, 'SemanticSimilarity': 0.8977791903714565}\n",
      "{'input_query': 'What are the marist gates built of?', 'LLMContextRecall': 0.7142857142857143, 'FactualCorrectness': 0.19, 'Faithfulness': 0.1, 'SemanticSimilarity': 0.9304953681969529}\n",
      "{'input_query': 'Benefit of FFE program?', 'LLMContextRecall': 1.0, 'FactualCorrectness': 0.31, 'Faithfulness': 0.5555555555555556, 'SemanticSimilarity': 0.926121290307824}\n",
      "{'input_query': 'When was Murray made president?', 'LLMContextRecall': 0.14285714285714285, 'FactualCorrectness': 0.13, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.911118887713052}\n",
      "{'input_query': 'Are there computers available to students in the library?', 'LLMContextRecall': 0.1, 'FactualCorrectness': 0.28, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.8386754303354413}\n",
      "{'input_query': 'Phone number Dr. Wermuth?', 'LLMContextRecall': 0.0, 'FactualCorrectness': 0.0, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.7584736795890439}\n",
      "{'input_query': 'When is the CAAS open?', 'LLMContextRecall': 0.6, 'FactualCorrectness': 0.4, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.9279693051296165}\n",
      "{'input_query': 'Is diversity embraced at Marist?', 'LLMContextRecall': 0.75, 'FactualCorrectness': 0.24, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.9180685254420928}\n",
      "{'input_query': 'What courses does Jill Wienbrock teach.', 'LLMContextRecall': 0.3333333333333333, 'FactualCorrectness': 0.33, 'Faithfulness': 0.75, 'SemanticSimilarity': 0.9275213818265705}\n",
      "{'input_query': '\\nDr. JAM', 'LLMContextRecall': 0.0, 'FactualCorrectness': 0.08, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.8106894098919385}\n",
      "{'input_query': 'What is SPPAC?', 'LLMContextRecall': 0.0, 'FactualCorrectness': 0.3, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.9531263189766641}\n",
      "{'input_query': 'Where did Dr. Lerner get his PHD?', 'LLMContextRecall': 1.0, 'FactualCorrectness': 0.07, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.860786387922315}\n",
      "{'input_query': 'department of philosphy and religious studies.', 'LLMContextRecall': 1.0, 'FactualCorrectness': 0.5, 'Faithfulness': 0.5, 'SemanticSimilarity': 0.9537212999672101}\n",
      "{'input_query': 'Marist Money refund', 'LLMContextRecall': 1.0, 'FactualCorrectness': 0.0, 'Faithfulness': 0.1111111111111111, 'SemanticSimilarity': 0.8870158479486756}\n",
      "{'input_query': 'Head of film photography marist', 'LLMContextRecall': 0.1, 'FactualCorrectness': 0.0, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.8202509246088014}\n",
      "{'input_query': 'Who published Displacements and Transformations in Caribbean Cultures?', 'LLMContextRecall': 0.75, 'FactualCorrectness': 0.25, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.847789880285623}\n",
      "{'input_query': 'Dr. Joseph Campisi bio?', 'LLMContextRecall': 1.0, 'FactualCorrectness': 0.65, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.9413944329610426}\n",
      "{'input_query': 'about Colleen Kopchik', 'LLMContextRecall': 1.0, 'FactualCorrectness': 0.76, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.9676557980257027}\n",
      "{'input_query': 'Undergrad admission Contact details for general enquires?\\n', 'LLMContextRecall': 0.0, 'FactualCorrectness': 0.32, 'Faithfulness': 0.5, 'SemanticSimilarity': 0.9304023420920371}\n",
      "{'input_query': 'What classes do you need for the gender studies pathway?', 'LLMContextRecall': 1.0, 'FactualCorrectness': 0.69, 'Faithfulness': 0.1, 'SemanticSimilarity': 0.8947046076876799}\n",
      "{'input_query': 'Where is the cab?', 'LLMContextRecall': 0.7, 'FactualCorrectness': 0.21, 'Faithfulness': 0.625, 'SemanticSimilarity': 0.850313142975442}\n",
      "{'input_query': \"what is Frieburghaus's education?\", 'LLMContextRecall': 0.625, 'FactualCorrectness': 0.04, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.9124873514402162}\n",
      "{'input_query': 'How can I find out more about the Clinical health counseling program?', 'LLMContextRecall': 0.7142857142857143, 'FactualCorrectness': 0.22, 'Faithfulness': 0.2, 'SemanticSimilarity': 0.9399750675027214}\n",
      "{'input_query': 'Who is Kathleen Boyle-LaBarbera', 'LLMContextRecall': 0.5, 'FactualCorrectness': 0.47, 'Faithfulness': 0.5, 'SemanticSimilarity': 0.9574928493994052}\n",
      "{'input_query': 'Tell me about Marist Civil Engagement.', 'LLMContextRecall': 0.9375, 'FactualCorrectness': 0.55, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.8527293174320609}\n",
      "{'input_query': 'info on Maureen Melita', 'LLMContextRecall': 0.9, 'FactualCorrectness': 0.51, 'Faithfulness': 0.9230769230769231, 'SemanticSimilarity': 0.9374778909820664}\n",
      "{'input_query': 'When did Henry Pratt come to Marist?', 'LLMContextRecall': 0.2, 'FactualCorrectness': 0.0, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.8511404806270793}\n",
      "{'input_query': 'What is the Mindset List?', 'LLMContextRecall': 1.0, 'FactualCorrectness': 0.3, 'Faithfulness': 0.6363636363636364, 'SemanticSimilarity': 0.9367041137969478}\n",
      "{'input_query': 'Are SAT/ACT scores required to apply to Marist?', 'LLMContextRecall': 0.5, 'FactualCorrectness': 0.26, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.8686647273148342}\n",
      "{'input_query': 'Whos the coordinator of Medieval studies?', 'LLMContextRecall': 0.4, 'FactualCorrectness': 0.2, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.9219675957118771}\n",
      "{'input_query': 'Who is the director of music?', 'LLMContextRecall': 1.0, 'FactualCorrectness': 0.0, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.8372556469973402}\n",
      "{'input_query': 'Suspicious emails', 'LLMContextRecall': 0.5714285714285714, 'FactualCorrectness': 0.11, 'Faithfulness': 0.625, 'SemanticSimilarity': 0.8441437480355363}\n",
      "{'input_query': 'Do I need to register my bike on campus?', 'LLMContextRecall': 0.29411764705882354, 'FactualCorrectness': 0.2, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.8470249771662303}\n",
      "{'input_query': 'Student financial services hours', 'LLMContextRecall': 0.6666666666666666, 'FactualCorrectness': 0.48, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.9173844053865473}\n",
      "{'input_query': 'Which Fire department will give response when emergency? ', 'LLMContextRecall': 0.3333333333333333, 'FactualCorrectness': 0.05, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.8952097167350657}\n",
      "{'input_query': 'Dana Jones', 'LLMContextRecall': 0.9, 'FactualCorrectness': 0.0, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.827312868572939}\n",
      "{'input_query': 'Which professor is associated with the rosevelt institute?', 'LLMContextRecall': 0.25, 'FactualCorrectness': 0.0, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.8372759808932233}\n",
      "{'input_query': 'How to use iLearn', 'LLMContextRecall': 0.9166666666666666, 'FactualCorrectness': 0.0, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.8617564594913087}\n",
      "{'input_query': 'How do you request an ally?', 'LLMContextRecall': 0.0, 'FactualCorrectness': 0.29, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.9069330499982614}\n",
      "{'input_query': 'Annamaria Maciocia\\n', 'LLMContextRecall': 1.0, 'FactualCorrectness': 0.2, 'Faithfulness': 0.125, 'SemanticSimilarity': 0.9332243089062717}\n",
      "{'input_query': 'Who is Leo Hall named after?', 'LLMContextRecall': 0.043478260869565216, 'FactualCorrectness': 0.22, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.8798223198864097}\n",
      "{'input_query': 'Undergrad parking', 'LLMContextRecall': 0.125, 'FactualCorrectness': 0.23, 'Faithfulness': 0.4, 'SemanticSimilarity': 0.8503885509669622}\n",
      "{'input_query': 'Where is the advising and academic services center?', 'LLMContextRecall': 0.875, 'FactualCorrectness': 0.38, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.9597000185354798}\n",
      "{'input_query': 'Does marist offer virtual tours?', 'LLMContextRecall': 0.75, 'FactualCorrectness': 0.2, 'Faithfulness': 0.3333333333333333, 'SemanticSimilarity': 0.8997053584293989}\n",
      "{'input_query': 'Bernstein memorial scholarship info', 'LLMContextRecall': 0.25, 'FactualCorrectness': 0.11, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.8821447140914093}\n",
      "{'input_query': 'Benefits of doing internships?', 'LLMContextRecall': 0.625, 'FactualCorrectness': 0.51, 'Faithfulness': 0.6923076923076923, 'SemanticSimilarity': 0.7922157656044357}\n",
      "{'input_query': 'Marist social media', 'LLMContextRecall': 1.0, 'FactualCorrectness': 0.1, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.8566347993515406}\n",
      "{'input_query': 'Dr. Gregory Machacek interests?', 'LLMContextRecall': 0.0, 'FactualCorrectness': 0.56, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.9274119133396952}\n",
      "{'input_query': 'What is the multicultural affairs center?', 'LLMContextRecall': 1.0, 'FactualCorrectness': 0.45, 'Faithfulness': 0.75, 'SemanticSimilarity': 0.9536253847037793}\n",
      "{'input_query': 'From where did Donise English graduate?', 'LLMContextRecall': 0.0, 'FactualCorrectness': 0.0, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.8532682074313823}\n",
      "{'input_query': 'What have Marist and IBM done together?', 'LLMContextRecall': 0.3333333333333333, 'FactualCorrectness': 0.36, 'Faithfulness': 0.6153846153846154, 'SemanticSimilarity': 0.9422144697983401}\n",
      "{'input_query': 'How to legitimize email', 'LLMContextRecall': 0.6153846153846154, 'FactualCorrectness': 0.58, 'Faithfulness': 0.42857142857142855, 'SemanticSimilarity': 0.853160599054918}\n",
      "{'input_query': 'Scams', 'LLMContextRecall': 0.29411764705882354, 'FactualCorrectness': 0.67, 'Faithfulness': 0.7, 'SemanticSimilarity': 0.829128890507008}\n",
      "{'input_query': 'What scholarships are available for Hudson Valley region students?', 'LLMContextRecall': 1.0, 'FactualCorrectness': 0.32, 'Faithfulness': 0.8, 'SemanticSimilarity': 0.882179462550077}\n",
      "{'input_query': 'What is the directors of the arts fashion foundation?', 'LLMContextRecall': 0.75, 'FactualCorrectness': 0.0, 'Faithfulness': 0.375, 'SemanticSimilarity': 0.8586697238508825}\n",
      "{'input_query': 'Who is Dan Mccormack?', 'LLMContextRecall': 0.9333333333333333, 'FactualCorrectness': 0.08, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.8520907809818052}\n",
      "{'input_query': 'how much is enrollment deposit?', 'LLMContextRecall': 0.7, 'FactualCorrectness': 0.5, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.9508966140650348}\n",
      "{'input_query': 'Who is the chair of the english department?', 'LLMContextRecall': 0.25, 'FactualCorrectness': 0.0, 'Faithfulness': 0.8, 'SemanticSimilarity': 0.80253522348834}\n",
      "{'input_query': 'Jobs for pyhsical therapy', 'LLMContextRecall': 1.0, 'FactualCorrectness': 0.21, 'Faithfulness': 0.0967741935483871, 'SemanticSimilarity': 0.8258440139903779}\n",
      "{'input_query': 'Tell me more about school of management', 'LLMContextRecall': 0.6666666666666666, 'FactualCorrectness': 0.17, 'Faithfulness': 0.2857142857142857, 'SemanticSimilarity': 0.8891518554342752}\n",
      "{'input_query': 'How can I learn about the term abroad program?', 'LLMContextRecall': 0.625, 'FactualCorrectness': 0.17, 'Faithfulness': 0.2857142857142857, 'SemanticSimilarity': 0.9214950379476655}\n",
      "{'input_query': 'Is Marist a diverse campus?', 'LLMContextRecall': 0.75, 'FactualCorrectness': 0.33, 'Faithfulness': 0.5454545454545454, 'SemanticSimilarity': 0.9213578321223894}\n",
      "{'input_query': 'What resources are available to me at the college?', 'LLMContextRecall': 1.0, 'FactualCorrectness': 0.27, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.9084108950712906}\n",
      "{'input_query': 'Who is Annamaria Maciocia', 'LLMContextRecall': 1.0, 'FactualCorrectness': 0.2, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.9332243089062717}\n",
      "{'input_query': 'Tell me about campus tours.', 'LLMContextRecall': 0.9, 'FactualCorrectness': 0.24, 'Faithfulness': 0.7777777777777778, 'SemanticSimilarity': 0.8916666840941503}\n",
      "{'input_query': 'Who is the honors program director?', 'LLMContextRecall': 0.6, 'FactualCorrectness': 0.0, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.8858262376102679}\n",
      "{'input_query': 'What is FOcus2?', 'LLMContextRecall': 0.0, 'FactualCorrectness': 0.25, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.8802651209357181}\n",
      "{'input_query': 'What is an internship?', 'LLMContextRecall': 0.5, 'FactualCorrectness': 0.35, 'Faithfulness': 0.5, 'SemanticSimilarity': 0.8661145676951627}\n",
      "{'input_query': 'Where can you do internships', 'LLMContextRecall': 0.6666666666666666, 'FactualCorrectness': 0.09, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.84009910423891}\n",
      "{'input_query': 'What educational areas does Marist give masters in?', 'LLMContextRecall': 0.3333333333333333, 'FactualCorrectness': 0.21, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.8676911702305523}\n",
      "{'input_query': 'Will I fall back on my curriculum if I go abroad?', 'LLMContextRecall': 0.5, 'FactualCorrectness': 0.1, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.8892734561345242}\n",
      "{'input_query': 'What classes does Dr. Sally Dwyer-McNulty teach?', 'LLMContextRecall': 0.6, 'FactualCorrectness': 0.55, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.9459592032712323}\n",
      "{'input_query': 'Who is Dr. Sally Dwyer-McNulty?', 'LLMContextRecall': 1.0, 'FactualCorrectness': 0.74, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.9606969416767979}\n",
      "{'input_query': 'How do a report a covid 19 issue on campus?', 'LLMContextRecall': 0.6666666666666666, 'FactualCorrectness': 0.06, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.9081566948640523}\n",
      "{'input_query': 'What is the distribution threshold that should be reached by students.\\n', 'LLMContextRecall': 0.07142857142857142, 'FactualCorrectness': 0.0, 'Faithfulness': 0.16666666666666666, 'SemanticSimilarity': 0.8553900653307586}\n",
      "{'input_query': 'Fanfarelli previous work', 'LLMContextRecall': 1.0, 'FactualCorrectness': 0.44, 'Faithfulness': 0.9, 'SemanticSimilarity': 0.9312425360999863}\n",
      "{'input_query': 'Who is Donise English?', 'LLMContextRecall': 0.75, 'FactualCorrectness': 0.13, 'Faithfulness': 0.5714285714285714, 'SemanticSimilarity': 0.9285818638896508}\n",
      "{'input_query': \"on what principles is the college's education grounded on?\", 'LLMContextRecall': 0.5714285714285714, 'FactualCorrectness': 0.38, 'Faithfulness': 0.75, 'SemanticSimilarity': 0.921577970677113}\n",
      "{'input_query': 'Does Marist has equipped with fire safety in all buildings?', 'LLMContextRecall': 1.0, 'FactualCorrectness': 0.12, 'Faithfulness': 0.625, 'SemanticSimilarity': 0.898243218824915}\n",
      "{'input_query': 'What is SNAP?', 'LLMContextRecall': 0.6, 'FactualCorrectness': 0.0, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.693549444074966}\n",
      "{'input_query': 'Career Fair', 'LLMContextRecall': 0.0, 'FactualCorrectness': 0.35, 'Faithfulness': 0.3076923076923077, 'SemanticSimilarity': 0.8907639332687618}\n",
      "{'input_query': 'MA clinical mental health', 'LLMContextRecall': 0.9285714285714286, 'FactualCorrectness': 0.34, 'Faithfulness': 0.125, 'SemanticSimilarity': 0.9378326156294516}\n",
      "{'input_query': 'What is the geographic profile of the Marist student body?', 'LLMContextRecall': 0.6, 'FactualCorrectness': 0.22, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.9042863475718487}\n",
      "{'input_query': 'Matt Frieburghaus about info', 'LLMContextRecall': 0.4166666666666667, 'FactualCorrectness': 0.24, 'Faithfulness': 0.75, 'SemanticSimilarity': 0.9339891993743576}\n",
      "{'input_query': 'Explain the Marist college core program.', 'LLMContextRecall': 1.0, 'FactualCorrectness': 0.58, 'Faithfulness': 0.4583333333333333, 'SemanticSimilarity': 0.9570730250836372}\n",
      "{'input_query': \"Who's the person to go to for Latin American and Caribbean Studies?\", 'LLMContextRecall': 1.0, 'FactualCorrectness': 0.0, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.852238497504554}\n",
      "{'input_query': 'Sigma Tau Delta advisor', 'LLMContextRecall': 0.25, 'FactualCorrectness': 0.16, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.8772378271779957}\n",
      "{'input_query': 'international student housing', 'LLMContextRecall': 0.1111111111111111, 'FactualCorrectness': 0.0, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.8727389931402405}\n",
      "{'input_query': \"What is Mark James Morreale's job?\", 'LLMContextRecall': 0.25, 'FactualCorrectness': 0.24, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.9242023551442426}\n",
      "{'input_query': 'Italian and Italian American studies include which courses and at which level?', 'LLMContextRecall': 0.38461538461538464, 'FactualCorrectness': 0.09, 'Faithfulness': 0.06666666666666667, 'SemanticSimilarity': 0.8824132351182631}\n",
      "{'input_query': 'Contact Career Services', 'LLMContextRecall': 0.2857142857142857, 'FactualCorrectness': 0.29, 'Faithfulness': 0.8888888888888888, 'SemanticSimilarity': 0.9416483718617243}\n",
      "{'input_query': \"What's the minimum gpa for the special education teacher program?\", 'LLMContextRecall': 0.5, 'FactualCorrectness': 0.15, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.8047943136784463}\n",
      "{'input_query': 'Where did Dr. Eden complete her PHD?', 'LLMContextRecall': 0.0, 'FactualCorrectness': 0.06, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.856109070312753}\n",
      "{'input_query': 'About Fanfarelli', 'LLMContextRecall': 1.0, 'FactualCorrectness': 0.5, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.956869028538299}\n",
      "{'input_query': 'When did women leadership start?', 'LLMContextRecall': 0.6, 'FactualCorrectness': 0.0, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.7792942573153336}\n",
      "{'input_query': 'What do I need to transfer to Marist', 'LLMContextRecall': 0.7708333333333334, 'FactualCorrectness': 0.39, 'Faithfulness': 0.875, 'SemanticSimilarity': 0.9281723890467791}\n",
      "{'input_query': 'DPT', 'LLMContextRecall': 0.7647058823529411, 'FactualCorrectness': 0.58, 'Faithfulness': 0.35714285714285715, 'SemanticSimilarity': 0.9354775863931333}\n",
      "{'input_query': 'Where is student financial services?', 'LLMContextRecall': 0.5, 'FactualCorrectness': 0.33, 'Faithfulness': 0.625, 'SemanticSimilarity': 0.9113257169633344}\n",
      "{'input_query': 'What does marist do for culture acceptance?', 'LLMContextRecall': 0.0, 'FactualCorrectness': 0.48, 'Faithfulness': 0.29411764705882354, 'SemanticSimilarity': 0.9207748901386441}\n",
      "{'input_query': 'What is the NRFSA?', 'LLMContextRecall': 1.0, 'FactualCorrectness': 0.0, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.7721758144560513}\n",
      "{'input_query': 'When was the tunnel made?', 'LLMContextRecall': 0.0, 'FactualCorrectness': 0.22, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.8774774297435757}\n",
      "{'input_query': 'How big was marist back in the day?', 'LLMContextRecall': 0.0, 'FactualCorrectness': 0.03, 'Faithfulness': 0.2, 'SemanticSimilarity': 0.8444538306919471}\n",
      "{'input_query': 'How many  home campus students study abroad each year?', 'LLMContextRecall': 0.8, 'FactualCorrectness': 0.2, 'Faithfulness': 0.42857142857142855, 'SemanticSimilarity': 0.9115997357362732}\n",
      "{'input_query': 'How to save money on tuition', 'LLMContextRecall': 0.64, 'FactualCorrectness': 0.0, 'Faithfulness': 0.09523809523809523, 'SemanticSimilarity': 0.8206201533639812}\n",
      "{'input_query': 'Where did Kathleen McNulty graduate from?\\n', 'LLMContextRecall': 0.6666666666666666, 'FactualCorrectness': 0.18, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.9331494084092948}\n",
      "{'input_query': 'can the the school help me find an internship or job?', 'LLMContextRecall': 1.0, 'FactualCorrectness': 0.27, 'Faithfulness': 0.375, 'SemanticSimilarity': 0.8935968650773956}\n",
      "{'input_query': 'Who is the chair of the Modern Languages and Cultures Department?', 'LLMContextRecall': 0.25, 'FactualCorrectness': 0.0, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.8264767469612325}\n",
      "{'input_query': \"What's the purpose of the Central gate?\", 'LLMContextRecall': 0.6666666666666666, 'FactualCorrectness': 0.32, 'Faithfulness': 0.8888888888888888, 'SemanticSimilarity': 0.9274378293359823}\n",
      "{'input_query': 'Is there a discount for classes for military personnel and their families?', 'LLMContextRecall': 0.5454545454545454, 'FactualCorrectness': 0.4, 'Faithfulness': 0.75, 'SemanticSimilarity': 0.9373665649489441}\n",
      "{'input_query': 'What does Desiree Dighton teach?', 'LLMContextRecall': 0.7777777777777778, 'FactualCorrectness': 0.12, 'Faithfulness': 0.3333333333333333, 'SemanticSimilarity': 0.9301917712632122}\n",
      "{'input_query': 'Duo security', 'LLMContextRecall': 0.42857142857142855, 'FactualCorrectness': 0.15, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.7756710246933917}\n",
      "{'input_query': 'Tell me about the president search.', 'LLMContextRecall': 0.9090909090909091, 'FactualCorrectness': 0.43, 'Faithfulness': 0.6, 'SemanticSimilarity': 0.9258788374597098}\n",
      "{'input_query': 'How do you get academic accommodation?', 'LLMContextRecall': 0.6, 'FactualCorrectness': 0.15, 'Faithfulness': 0.13333333333333333, 'SemanticSimilarity': 0.9078519333442069}\n",
      "{'input_query': 'Cost of credit for Masters in Integrated Marketing Communication', 'LLMContextRecall': 0.2857142857142857, 'FactualCorrectness': 0.13, 'Faithfulness': 0.42857142857142855, 'SemanticSimilarity': 0.909033679216831}\n",
      "{'input_query': 'Is IMC degree online?', 'LLMContextRecall': 0.8, 'FactualCorrectness': 0.23, 'Faithfulness': 0.5, 'SemanticSimilarity': 0.9304079258254312}\n",
      "{'input_query': \"Where can I read about students' term abroad experiences?\", 'LLMContextRecall': 0.7142857142857143, 'FactualCorrectness': 0.22, 'Faithfulness': 0.5714285714285714, 'SemanticSimilarity': 0.9036070528127736}\n",
      "{'input_query': 'What are the campus ministry hours?', 'LLMContextRecall': 0.35714285714285715, 'FactualCorrectness': 0.07, 'Faithfulness': 0.3333333333333333, 'SemanticSimilarity': 0.8901724854672672}\n",
      "{'input_query': 'Location and office hours of student financial services at Marist?', 'LLMContextRecall': 0.25, 'FactualCorrectness': 0.27, 'Faithfulness': 0.8333333333333334, 'SemanticSimilarity': 0.8879951693905495}\n",
      "{'input_query': 'Dr. Stephen M. Mercier', 'LLMContextRecall': 0.7333333333333333, 'FactualCorrectness': 0.28, 'Faithfulness': 0.6666666666666666, 'SemanticSimilarity': 0.892986143883794}\n",
      "{'input_query': \"Does Marist's pre-college program offer any academic resources?\", 'LLMContextRecall': 0.5555555555555556, 'FactualCorrectness': 0.12, 'Faithfulness': 0.8, 'SemanticSimilarity': 0.9082416440179759}\n",
      "{'input_query': 'Where can I see information regarding my federal loans?', 'LLMContextRecall': 0.5, 'FactualCorrectness': 0.08, 'Faithfulness': 0.25, 'SemanticSimilarity': 0.8101520305703364}\n",
      "{'input_query': 'Who is Goletti?', 'LLMContextRecall': 0.09090909090909091, 'FactualCorrectness': 0.0, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.7868257214172271}\n",
      "{'input_query': 'Fashion program director', 'LLMContextRecall': 0.5, 'FactualCorrectness': 0.35, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.9534981731302145}\n",
      "{'input_query': \"Michael E. O'Sullivan\", 'LLMContextRecall': 0.9, 'FactualCorrectness': 0.04, 'Faithfulness': 0.16666666666666666, 'SemanticSimilarity': 0.8852534978544635}\n",
      "{'input_query': 'Cost of credit for Master of Business Administration? ', 'LLMContextRecall': 0.11764705882352941, 'FactualCorrectness': 0.14, 'Faithfulness': 0.5, 'SemanticSimilarity': 0.9098636236297223}\n",
      "{'input_query': 'What is the Honors program?', 'LLMContextRecall': 1.0, 'FactualCorrectness': 0.53, 'Faithfulness': 0.5, 'SemanticSimilarity': 0.9451271020420728}\n",
      "{'input_query': 'What can you buy with Marist Money?', 'LLMContextRecall': 0.5714285714285714, 'FactualCorrectness': 0.51, 'Faithfulness': 0.8571428571428571, 'SemanticSimilarity': 0.8978807215116026}\n",
      "{'input_query': 'What does OHBM do?', 'LLMContextRecall': 0.21428571428571427, 'FactualCorrectness': 0.0, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.7236511206166836}\n",
      "{'input_query': 'What are the academic schools at Marist?', 'LLMContextRecall': 0.1, 'FactualCorrectness': 0.16, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.8417781256991137}\n",
      "{'input_query': 'Who made the strategic plan?', 'LLMContextRecall': 0.0, 'FactualCorrectness': 0.08, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.9203188703459637}\n",
      "{'input_query': 'What are most popular Internship companies for  MBA students?', 'LLMContextRecall': 0.5555555555555556, 'FactualCorrectness': 0.0, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.8377961471346371}\n",
      "{'input_query': 'Can the college help me plan for graduate school?', 'LLMContextRecall': 0.5, 'FactualCorrectness': 0.05, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.8288094842911987}\n",
      "{'input_query': 'When was the tunnel made?', 'LLMContextRecall': 0.0, 'FactualCorrectness': 0.09, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.7354636362175581}\n",
      "{'input_query': 'When are computer science classes held?', 'LLMContextRecall': 0.8, 'FactualCorrectness': 0.44, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.8796066178214024}\n",
      "{'input_query': 'How many people are on the presidential search team?', 'LLMContextRecall': 0.6, 'FactualCorrectness': 0.0, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.822635556116123}\n",
      "{'input_query': 'Where is student financial services?', 'LLMContextRecall': 0.0, 'FactualCorrectness': 0.27, 'Faithfulness': 0.3333333333333333, 'SemanticSimilarity': 0.9016173363306491}\n",
      "{'input_query': 'Computer Store', 'LLMContextRecall': 0.0, 'FactualCorrectness': 0.75, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.9329337608962671}\n",
      "{'input_query': 'Do we have to take the GRE for graduate school?', 'LLMContextRecall': 0.3333333333333333, 'FactualCorrectness': 0.15, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.8032592353368188}\n",
      "{'input_query': 'Various internship opportunities for school of science students?\\n', 'LLMContextRecall': 0.3, 'FactualCorrectness': 0.21, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.8343784742284527}\n",
      "{'input_query': 'Location and office hours of student financial services at Marist?', 'LLMContextRecall': 0.25, 'FactualCorrectness': 0.19, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.8820382030279252}\n",
      "{'input_query': 'tell me about marist counseling', 'LLMContextRecall': 1.0, 'FactualCorrectness': 0.38, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.918007483726478}\n",
      "{'input_query': 'Undergrad admission department hours', 'LLMContextRecall': 0.5, 'FactualCorrectness': 0.2, 'Faithfulness': 0.5, 'SemanticSimilarity': 0.9098991887723198}\n",
      "{'input_query': 'music scholarship award', 'LLMContextRecall': 0.8333333333333334, 'FactualCorrectness': 0.1, 'Faithfulness': 0.2857142857142857, 'SemanticSimilarity': 0.8935331491173999}\n",
      "{'input_query': 'What are most popular Internship companies for  MBA students?', 'LLMContextRecall': 0.5555555555555556, 'FactualCorrectness': 0.02, 'Faithfulness': 0.5, 'SemanticSimilarity': 0.873560835358821}\n",
      "{'input_query': 'What is NSF REU?', 'LLMContextRecall': 0.05555555555555555, 'FactualCorrectness': 0.23, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.8205262591678175}\n",
      "{'input_query': 'Purchase textbooks for class.', 'LLMContextRecall': 0.3333333333333333, 'FactualCorrectness': 0.21, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.879270817402597}\n",
      "{'input_query': 'Who is Kathleen Boyle-LaBarbera', 'LLMContextRecall': 0.7272727272727273, 'FactualCorrectness': 0.0, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.8135187142424856}\n",
      "{'input_query': 'about Dr. Schrier', 'LLMContextRecall': 0.9, 'FactualCorrectness': 0.44, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.9511286179623085}\n",
      "{'input_query': 'When did Henry Pratt come to Marist?', 'LLMContextRecall': 0.2, 'FactualCorrectness': 0.0, 'Faithfulness': 0.6666666666666666, 'SemanticSimilarity': 0.8131289879311562}\n",
      "{'input_query': 'Dr. Stephen M. Mercier', 'LLMContextRecall': 0.7333333333333333, 'FactualCorrectness': 0.24, 'Faithfulness': 0.75, 'SemanticSimilarity': 0.9086285934113685}\n",
      "{'input_query': 'When are flu shots due by?', 'LLMContextRecall': 0.5, 'FactualCorrectness': 0.0, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.7175843089641987}\n",
      "{'input_query': 'The MSPAccy program has how many credits for full time classes and  also during fall or spring ?', 'LLMContextRecall': 1.0, 'FactualCorrectness': 0.2, 'Faithfulness': 0.5, 'SemanticSimilarity': 0.9221481821294506}\n",
      "{'input_query': 'Who is Ed Smith Marist', 'LLMContextRecall': 0.0, 'FactualCorrectness': 0.0, 'Faithfulness': 0.5, 'SemanticSimilarity': 0.8041918056011781}\n",
      "{'input_query': 'What are MBA courses like?', 'LLMContextRecall': 0.8888888888888888, 'FactualCorrectness': 0.32, 'Faithfulness': 0.3333333333333333, 'SemanticSimilarity': 0.9077362665294226}\n",
      "{'input_query': 'What does Prodessor Scepanski teach?', 'LLMContextRecall': 0.8, 'FactualCorrectness': 0.0, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.7953221590276973}\n",
      "{'input_query': 'Marist contribution in Internships?', 'LLMContextRecall': 1.0, 'FactualCorrectness': 0.07, 'Faithfulness': 0.6, 'SemanticSimilarity': 0.9174084155541081}\n",
      "{'input_query': 'What can you buy with Marist Money?', 'LLMContextRecall': 0.9, 'FactualCorrectness': 0.44, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.8961946999387393}\n",
      "{'input_query': 'Can you combine history and education?', 'LLMContextRecall': 1.0, 'FactualCorrectness': 0.06, 'Faithfulness': 0.6, 'SemanticSimilarity': 0.900264395899567}\n",
      "{'input_query': 'Is there capping in MAEP?', 'LLMContextRecall': 0.8, 'FactualCorrectness': 0.0, 'Faithfulness': 0.3333333333333333, 'SemanticSimilarity': 0.8157170801947112}\n",
      "{'input_query': 'Where did Dr. Tarantello go to school for her phd?', 'LLMContextRecall': 0.5, 'FactualCorrectness': 0.12, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.9183631567584859}\n",
      "{'input_query': 'what does the academic learning center do?', 'LLMContextRecall': 1.0, 'FactualCorrectness': 0.45, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.9349200631406588}\n",
      "{'input_query': \"What are the hours of the registrar's office?\", 'LLMContextRecall': 0.5294117647058824, 'FactualCorrectness': 0.08, 'Faithfulness': 0.8333333333333334, 'SemanticSimilarity': 0.8940117033083737}\n",
      "{'input_query': 'Matt Frieburghaus about info', 'LLMContextRecall': 0.8888888888888888, 'FactualCorrectness': 0.24, 'Faithfulness': 0.9166666666666666, 'SemanticSimilarity': 0.9318663466896571}\n",
      "{'input_query': 'What is the language center?', 'LLMContextRecall': 0.7, 'FactualCorrectness': 0.39, 'Faithfulness': 0.6, 'SemanticSimilarity': 0.9278576607320578}\n",
      "{'input_query': 'Marist College campus size', 'LLMContextRecall': 0.0, 'FactualCorrectness': 0.13, 'Faithfulness': 0.375, 'SemanticSimilarity': 0.9071051062075703}\n",
      "{'input_query': 'What is ATI?', 'LLMContextRecall': 0.4, 'FactualCorrectness': 0.0, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.7487386200786559}\n",
      "{'input_query': 'Lori Beth research interests', 'LLMContextRecall': 0.8181818181818182, 'FactualCorrectness': 0.0, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.809361100405364}\n",
      "{'input_query': 'What does ICA stand for?', 'LLMContextRecall': 0.18181818181818182, 'FactualCorrectness': 0.0, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.7767732416168897}\n",
      "{'input_query': 'What classes do you need for the gender studies pathway?', 'LLMContextRecall': 1.0, 'FactualCorrectness': 0.71, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.8984821346229981}\n",
      "{'input_query': 'Who is Dr. Lynn Eckert?', 'LLMContextRecall': 0.8888888888888888, 'FactualCorrectness': 0.5, 'Faithfulness': 0.5, 'SemanticSimilarity': 0.9542064619310258}\n",
      "{'input_query': 'In which time period on- campus housing is available?', 'LLMContextRecall': 0.6, 'FactualCorrectness': 0.17, 'Faithfulness': 0.5, 'SemanticSimilarity': 0.8030266949979794}\n",
      "LLMContextRecall - Mean: 0.5467634339297106, St. Dev: 0.3360740003541348\n",
      "FactualCorrectness - Mean: 0.2235114503816794, St. Dev: 0.18901683621494025\n",
      "Faithfulness - Mean: 0.43729878882441997, St. Dev: 0.4023429210440114\n",
      "SemanticSimilarity - Mean: 0.8807487732359173, St. Dev: 0.05687518194060975\n"
     ]
    }
   ],
   "source": [
    "# Run our chain with each question and evaluate\n",
    "chatHistory = None\n",
    "for row in maristTestSample.iterrows():\n",
    "    chatHistory = simpleRAG(row[1][0], chatHistory, row[1][1])\n",
    "\n",
    "## Evaluation\n",
    "evalResults, metricStats = await pipelineEvaluation(evaluationSamples, evalMetrics)\n",
    "for result in evalResults:\n",
    "    print(result)\n",
    "\n",
    "for metric in metricStats.keys():\n",
    "    print(f\"{metric} - Mean: {metricStats[metric]['mean']}, St. Dev: {metricStats[metric]['std_dev']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM-LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
