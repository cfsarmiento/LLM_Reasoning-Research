{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70577418",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Simple RAG Implementation\n",
    "Author: Christian Sarmiento\n",
    "Purpose: This notebook is intended to get a simple implementation of RAG set up with LangChain.\n",
    "Date Created: 10/1/24\n",
    "Last Updated: 10/1/24\n",
    "Data: https://archive.ics.uci.edu/dataset/450/sports+articles+for+objectivity+analysis\n",
    "Sources:\n",
    "- https://python.langchain.com/docs/tutorials/rag/\n",
    "- https://python.langchain.com/docs/tutorials/llm_chain/\n",
    "- https://medium.com/@dinabavli/rag-basics-basic-implementation-of-retrieval-augmented-generation-rag-e80e0791159d\n",
    "- ChatGPT: o1-preview\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "RAG Research             |               Machine Learning Independent Study             |              DR. EITEL LAURIA\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f6dfada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "sys.path.append(\"/Users/christiansarmiento/Library/CloudStorage/OneDrive-MaristCollege/Machine Learning/Private Code\")\n",
    "from api_keys import openAIKey\n",
    "from api_keys import langchainKey\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain import hub  # for RAG prompt\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6cc71a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain Enviornment Variables\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = langchainKey()\n",
    "os.environ[\"OPENAI_API_KEY\"] = openAIKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99387228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OpenAI model\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc182a12-6a70-4fb9-8f77-823e341f989a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "folderPath = \"/Users/christiansarmiento/Library/CloudStorage/OneDrive-MaristCollege/Machine Learning/Data/sports_articles_corpus/Raw data\"\n",
    "sportsArticles = []\n",
    "\n",
    "for fileName in os.listdir(folderPath):\n",
    "    filePath = os.path.join(folderPath, fileName)\n",
    "    loader = TextLoader(filePath, encoding='latin1')  # UTF-8 not working for the files\n",
    "    doc = loader.load()\n",
    "    sportsArticles.extend(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c567c173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Documents into Chunks\n",
    "textSplitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, add_start_index=True)\n",
    "texts = textSplitter.split_documents(sportsArticles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2646002f-c802-441e-bd43-adb3bb76c2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store Documents in Vector DB (Chroma)\n",
    "vectorDB = Chroma.from_documents(documents=texts, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac4c8790-f634-40b7-9d96-841c65f5f9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Retrieval System\n",
    "retriever = vectorDB.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})  # Retrieves 3 documents\n",
    "\n",
    "# To get retrieved documents:\n",
    "# retrievedDocuments = retriever.invoke(\"query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1f6b342-08b9-4e63-b946-280e3590ac85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the RAG Chain\n",
    "\n",
    "# Function to format documents into the prompt\n",
    "def formatDocs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Setup RAG Chain\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "ragChain = (\n",
    "    {\"context\": retriever | formatDocs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24e7ebd5-e639-491d-aaaf-636cd788b166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The offside rule in soccer states that a player is in an offside position if they are nearer to the opponent's goal line than both the ball and the second-to-last opponent when the ball is played to them, unless they are in their own half or level with the second-to-last opponent. Being in an offside position is not an offense in itself; the player must become involved in active play to be penalized. The rule aims to prevent players from gaining an unfair advantage by lingering near the opponent's goal."
     ]
    }
   ],
   "source": [
    "# Results\n",
    "for chunk in ragChain.stream(\"Explain the offside rule in soccer.\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f88a596-3c50-43af-9ca5-bcb13feb4570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if answers are coming from the llm or from the documents\n",
    "# Try giving documents that aren't real then asking questions on things off of that\n",
    "# Avoids the model relying on trained info \n",
    "# Play with the system prompt\n",
    "\n",
    "# Next step after QA - feed answers into the system to make it more conversational\n",
    "# Implement Gradio\n",
    "# Knowledge Graph \n",
    "# Identifying Metrics - do research!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d5d2da-5ee6-4dd7-8008-8ee8a751d297",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Implement with Marist Data\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49d2eaa0-434f-409a-8299-d94c24d4f207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "csvPath = \"/Users/christiansarmiento/Library/CloudStorage/OneDrive-MaristCollege/Machine Learning/Data/Marist_QA.csv\"\n",
    "maristQA = pd.read_csv(csvPath, header=None)\n",
    "\n",
    "# To use RecursiveCharacterTextSplitter, we need a list of dictionaries\n",
    "maristContext = [Document(page_content=text) for text in maristQA[1].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e06dbe67-e698-4083-8998-370ff78fc21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Documents into Chunks\n",
    "textSplitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, add_start_index=True)\n",
    "texts = textSplitter.split_documents(maristContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dac6137-0ecf-4c22-9c6a-ec8c86c6a134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store Documents in Vector DB (Chroma)\n",
    "vectorDB = Chroma.from_documents(documents=texts, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cfc0f3a-3933-4dd7-8558-d9b240a83e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Retrieval System\n",
    "retriever = vectorDB.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})  # Retrieves 3 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd26cb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts\n",
    "systemPrompt = (\n",
    "    \n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    "    \n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    \n",
    "    [\n",
    "        (\"system\", systemPrompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    "    \n",
    ")\n",
    "\n",
    "contextualizeSystemPrompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualizePrompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualizeSystemPrompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "historyAwareRetriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualizePrompt\n",
    ")\n",
    "\n",
    "qaPrompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", systemPrompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7dad046b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make chains\n",
    "questionAnswerChain = create_stuff_documents_chain(llm, qaPrompt)\n",
    "ragChain = create_retrieval_chain(historyAwareRetriever, questionAnswerChain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e91b900d-6610-46a6-be4b-4c76a99e70f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marist College is located on the banks of the Hudson River and also has a campus in Florence, Italy.\n",
      "\n",
      "Marist College is in Poughkeepsie, New York, situated along the Hudson River. The Florence campus is located in Florence, Italy.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Talking to ChatGPT, making RAG conversational\n",
    "conversationHistory = []\n",
    "userQuery = input(\"Prompt (0 to quit): \")\n",
    "while userQuery != '0':\n",
    "\n",
    "    # Print input - this is just for a VSCode enviornment to see I/O together, feel free to comment out in Jupyter\n",
    "    print(f\"User: {userQuery}\")\n",
    "\n",
    "    # Call ChatGPT using RAG chain\n",
    "    llmResponse = ragChain.invoke({\"input\": userQuery, \"chat_history\": conversationHistory})\n",
    "    print(f\"LLM: {llmResponse['answer']}\")\n",
    "    print()\n",
    "    conversationHistory.extend([\n",
    "        \n",
    "        HumanMessage(content=userQuery),\n",
    "        AIMessage(content=llmResponse[\"answer\"]),\n",
    "    ])\n",
    "\n",
    "    # New prompt\n",
    "    userQuery = input(\"Prompt (0 to quit): \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
