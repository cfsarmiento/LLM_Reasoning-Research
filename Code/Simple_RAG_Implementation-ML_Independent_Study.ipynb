{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70577418",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Simple RAG Implementation\n",
    "Author: Christian Sarmiento\n",
    "Purpose: This notebook is intended to get a simple implementation of RAG set up with LangChain.\n",
    "Date Created: 10/1/24\n",
    "Last Updated: 11/28/24\n",
    "Data: https://archive.ics.uci.edu/dataset/450/sports+articles+for+objectivity+analysis\n",
    "Sources:\n",
    "- https://python.langchain.com/docs/tutorials/rag/\n",
    "- https://python.langchain.com/docs/tutorials/llm_chain/\n",
    "- https://medium.com/@dinabavli/rag-basics-basic-implementation-of-retrieval-augmented-generation-rag-e80e0791159d\n",
    "- ChatGPT: o1-preview\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "RAG Research             |               Machine Learning Independent Study             |              DR. EITEL LAURIA\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "db84d629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ragas in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (0.2.3)\n",
      "Requirement already satisfied: numpy in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from ragas) (1.26.4)\n",
      "Requirement already satisfied: datasets in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from ragas) (3.1.0)\n",
      "Requirement already satisfied: tiktoken in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from ragas) (0.7.0)\n",
      "Requirement already satisfied: langchain in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from ragas) (0.3.1)\n",
      "Requirement already satisfied: langchain-core in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from ragas) (0.3.6)\n",
      "Requirement already satisfied: langchain-community in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from ragas) (0.3.1)\n",
      "Requirement already satisfied: langchain-openai in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from ragas) (0.2.1)\n",
      "Requirement already satisfied: nest-asyncio in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from ragas) (1.6.0)\n",
      "Requirement already satisfied: appdirs in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from ragas) (1.4.4)\n",
      "Requirement already satisfied: pydantic>=2 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from ragas) (2.9.2)\n",
      "Requirement already satisfied: openai>1 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from ragas) (1.50.0)\n",
      "Requirement already satisfied: pysbd>=0.3.4 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from ragas) (0.3.4)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from openai>1->ragas) (4.6.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from openai>1->ragas) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from openai>1->ragas) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from openai>1->ragas) (0.5.0)\n",
      "Requirement already satisfied: sniffio in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from openai>1->ragas) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from openai>1->ragas) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from openai>1->ragas) (4.11.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from pydantic>=2->ragas) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from pydantic>=2->ragas) (2.23.4)\n",
      "Requirement already satisfied: filelock in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from datasets->ragas) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from datasets->ragas) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from datasets->ragas) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from datasets->ragas) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from datasets->ragas) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from datasets->ragas) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from datasets->ragas) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets->ragas) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from datasets->ragas) (3.10.6)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from datasets->ragas) (0.25.1)\n",
      "Requirement already satisfied: packaging in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from datasets->ragas) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from datasets->ragas) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from langchain->ragas) (2.0.35)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from langchain->ragas) (0.3.0)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from langchain->ragas) (0.1.129)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from langchain->ragas) (8.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from langchain-core->ragas) (1.33)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from langchain-community->ragas) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from langchain-community->ragas) (2.5.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from tiktoken->ragas) (2024.9.11)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from aiohttp->datasets->ragas) (2.4.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from aiohttp->datasets->ragas) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from aiohttp->datasets->ragas) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from aiohttp->datasets->ragas) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from aiohttp->datasets->ragas) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from aiohttp->datasets->ragas) (1.13.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai>1->ragas) (3.10)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (3.22.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (0.9.0)\n",
      "Requirement already satisfied: certifi in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai>1->ragas) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai>1->ragas) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>1->ragas) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core->ragas) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain->ragas) (3.10.7)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community->ragas) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from requests>=2.32.2->datasets->ragas) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from requests>=2.32.2->datasets->ragas) (2.2.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain->ragas) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from pandas->datasets->ragas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from pandas->datasets->ragas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from pandas->datasets->ragas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets->ragas) (1.16.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: nltk in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from nltk) (4.66.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Download RAGAS for RAG metrics\n",
    "%pip install ragas\n",
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f6dfada",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in _VertexAIBase has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in _VertexAICommon has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import sys\n",
    "sys.path.append(\"/Users/christiansarmiento/Library/CloudStorage/OneDrive-MaristCollege/Machine Learning/Private Code\")\n",
    "from api_keys import openAIKey\n",
    "from api_keys import langchainKey\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain import hub  # for RAG prompt\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import pandas as pd\n",
    "import os\n",
    "import gradio as gr  # easy frontend implementation\n",
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, SemanticSimilarity\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas import SingleTurnSample, EvaluationDataset\n",
    "from ragas import evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6cc71a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain Enviornment Variables\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = langchainKey()\n",
    "os.environ[\"OPENAI_API_KEY\"] = openAIKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99387228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OpenAI model with metric wrapper\n",
    "#llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
    "#evalEmbeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc182a12-6a70-4fb9-8f77-823e341f989a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "folderPath = \"/Users/christiansarmiento/Library/CloudStorage/OneDrive-MaristCollege/Machine Learning/Data/sports_articles_corpus/Raw data\"\n",
    "sportsArticles = []\n",
    "\n",
    "for fileName in os.listdir(folderPath):\n",
    "    filePath = os.path.join(folderPath, fileName)\n",
    "    loader = TextLoader(filePath, encoding='latin1')  # UTF-8 not working for the files\n",
    "    doc = loader.load()\n",
    "    sportsArticles.extend(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c567c173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Documents into Chunks\n",
    "textSplitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, add_start_index=True)\n",
    "texts = textSplitter.split_documents(sportsArticles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2646002f-c802-441e-bd43-adb3bb76c2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store Documents in Vector DB (Chroma)\n",
    "vectorDB = Chroma.from_documents(documents=texts, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac4c8790-f634-40b7-9d96-841c65f5f9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Retrieval System\n",
    "retriever = vectorDB.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})  # Retrieves 3 documents\n",
    "\n",
    "# To get retrieved documents:\n",
    "# retrievedDocuments = retriever.invoke(\"query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f6b342-08b9-4e63-b946-280e3590ac85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the RAG Chain\n",
    "\n",
    "# Function to format documents into the prompt\n",
    "def formatDocs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Setup RAG Chain\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "ragChain = (\n",
    "    {\"context\": retriever | formatDocs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24e7ebd5-e639-491d-aaaf-636cd788b166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The offside rule in soccer states that a player is in an offside position if they are nearer to the opponent's goal line than both the ball and the second-to-last opponent when the ball is played to them, unless they are in their own half or level with the second-to-last opponent. Being in an offside position is not an offense in itself; the player must become involved in active play to be penalized. The rule aims to prevent players from gaining an unfair advantage by lingering near the opponent's goal."
     ]
    }
   ],
   "source": [
    "# Results\n",
    "for chunk in ragChain.stream(\"Explain the offside rule in soccer.\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f88a596-3c50-43af-9ca5-bcb13feb4570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if answers are coming from the llm or from the documents\n",
    "# Try giving documents that aren't real then asking questions on things off of that\n",
    "# Avoids the model relying on trained info \n",
    "# Play with the system prompt\n",
    "\n",
    "# Next step after QA - feed answers into the system to make it more conversational\n",
    "# Implement Gradio\n",
    "# Knowledge Graph \n",
    "# Identifying Metrics - do research!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d5d2da-5ee6-4dd7-8008-8ee8a751d297",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Implement with Marist Data\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49d2eaa0-434f-409a-8299-d94c24d4f207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "csvPath = \"/Users/christiansarmiento/Library/CloudStorage/OneDrive-MaristCollege/Machine Learning/Data/Marist_QA.csv\"\n",
    "maristQA = pd.read_csv(csvPath, header=None)\n",
    "\n",
    "# To use RecursiveCharacterTextSplitter, we need a list of dictionaries\n",
    "maristContext = [Document(page_content=text) for text in maristQA[1].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e06dbe67-e698-4083-8998-370ff78fc21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Documents into Chunks\n",
    "textSplitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, add_start_index=True)\n",
    "texts = textSplitter.split_documents(maristContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dac6137-0ecf-4c22-9c6a-ec8c86c6a134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store Documents in Vector DB (Chroma)\n",
    "vectorDB = Chroma.from_documents(documents=texts, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cfc0f3a-3933-4dd7-8558-d9b240a83e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Retrieval System\n",
    "retriever = vectorDB.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})  # Retrieves 3 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd26cb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts\n",
    "systemPrompt = (\n",
    "    \n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    "    \n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    \n",
    "    [\n",
    "        (\"system\", systemPrompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    "    \n",
    ")\n",
    "\n",
    "contextualizeSystemPrompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualizePrompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualizeSystemPrompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "historyAwareRetriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualizePrompt\n",
    ")\n",
    "\n",
    "qaPrompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", systemPrompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dad046b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make chains\n",
    "questionAnswerChain = create_stuff_documents_chain(llm, qaPrompt)\n",
    "ragChain = create_retrieval_chain(historyAwareRetriever, questionAnswerChain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e91b900d-6610-46a6-be4b-4c76a99e70f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marist College is located on the banks of the Hudson River and also has a campus in Florence, Italy.\n",
      "\n",
      "Marist College is in Poughkeepsie, New York, situated along the Hudson River. The Florence campus is located in Florence, Italy.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Talking to ChatGPT, making RAG conversational\n",
    "conversationHistory = []\n",
    "userQuery = input(\"Prompt (0 to quit): \")\n",
    "while userQuery != '0':\n",
    "\n",
    "    # Print input - this is just for a VSCode enviornment to see I/O together, feel free to comment out in Jupyter\n",
    "    print(f\"User: {userQuery}\")\n",
    "\n",
    "    # Call ChatGPT using RAG chain\n",
    "    llmResponse = ragChain.invoke({\"input\": userQuery, \"chat_history\": conversationHistory})\n",
    "    print(f\"LLM: {llmResponse['answer']}\")\n",
    "    print()\n",
    "    conversationHistory.extend([\n",
    "        \n",
    "        HumanMessage(content=userQuery),\n",
    "        AIMessage(content=llmResponse[\"answer\"]),\n",
    "        \n",
    "    ])\n",
    "\n",
    "    # New prompt\n",
    "    userQuery = input(\"Prompt (0 to quit): \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39c79824",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages/gradio/components/chatbot.py:222: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6g/8868nrgn5znbjrhdbycnn8pw0000gn/T/ipykernel_36051/1194288707.py:18: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrievedContexts = [context.page_content for context in retriever.get_relevant_documents(userQuery)]\n"
     ]
    }
   ],
   "source": [
    "# Frontend with Gradio\n",
    "\n",
    "'''\n",
    "Function that calls the RAG chain for Gradio\n",
    "'''\n",
    "evaluationSamples = []\n",
    "def simpleRAG(userQuery, history, correctAnswer):\n",
    "\n",
    "    # Ensure there is a list to use for the conversation history\n",
    "    if history is None:\n",
    "        history = []\n",
    "\n",
    "    # Call ChatGPT using RAG chain\n",
    "    llmResponse = ragChain.invoke({\"input\": userQuery, \"chat_history\": history})\n",
    "\n",
    "    # Get response and context for evaluation\n",
    "    responseText = llmResponse[\"answer\"]\n",
    "    retrievedContexts = [context.page_content for context in retriever.get_relevant_documents(userQuery)]\n",
    "\n",
    "    # Save information for RAG metrics\n",
    "    evaluationSamples.append({\n",
    "        \"user_input\": userQuery,\n",
    "        \"retrieved_contexts\": retrievedContexts,\n",
    "        \"response\": responseText,\n",
    "        \"reference\": correctAnswer    # ground truth \n",
    "    })\n",
    "\n",
    "    # Save chat history for conversational aspect\n",
    "    history.extend([\n",
    "        \n",
    "        HumanMessage(content=userQuery),\n",
    "        AIMessage(content=llmResponse[\"answer\"]),\n",
    "        \n",
    "    ])\n",
    "\n",
    "    # Save input and output to history\n",
    "    history.append(HumanMessage(content=userQuery))\n",
    "    history.append(AIMessage(content=llmResponse[\"answer\"]))\n",
    "\n",
    "    # Prepare display of data\n",
    "    chatDisplay = [(msg.content, \"User\" if isinstance(msg, HumanMessage) else \"LLM\") for msg in history]\n",
    "\n",
    "    return chatDisplay, history\n",
    "\n",
    "# Frontend\n",
    "interface = gr.Interface(\n",
    "    fn=simpleRAG,  \n",
    "    inputs=[\"text\", \"state\", gr.Textbox(label=\"Correct Answer\")],  \n",
    "    outputs=[\"chatbot\", \"state\"],  \n",
    "    title=\"Simple RAG\",  \n",
    "    description=\"Initial setup for a simple conversational RAG process.\"\n",
    ")\n",
    "\n",
    "# Launch the frontend\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e13b080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate our RAG pipeline\n",
    "async def pipelineEvaluation(dataset, metrics):\n",
    "\n",
    "    # Run through our runs\n",
    "    results = []\n",
    "    for run in dataset:\n",
    "\n",
    "        # Save our inputs/outputs\n",
    "        inputQuery = run[\"user_input\"]\n",
    "        groundTruthAnswer = run[\"reference\"]\n",
    "        contexts = run[\"retrieved_contexts\"]\n",
    "        response = run[\"response\"]\n",
    "\n",
    "        # Create a SingleTurnSample object\n",
    "        sample = SingleTurnSample(\n",
    "            user_input=inputQuery,\n",
    "            response=response,\n",
    "            reference=groundTruthAnswer,\n",
    "            retrieved_contexts=contexts \n",
    "        )\n",
    "\n",
    "        # Evaluate metrics\n",
    "        runResults = {\"input_query\": inputQuery}\n",
    "        for metric in metrics:\n",
    "\n",
    "            # Get the score for the given metric\n",
    "            try:\n",
    "\n",
    "                score = await metric.single_turn_ascore(sample)\n",
    "                runResults[type(metric).__name__] = score\n",
    "\n",
    "            except Exception as e:\n",
    "                # Catch errors for debugging\n",
    "                runResults[type(metric).__name__] = f\"Error: {str(e)}\"\n",
    "        \n",
    "        # Save metric results\n",
    "        results.append(runResults)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce47ce73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metrics\n",
    "evalMetrics = [LLMContextRecall(llm=LangchainLLMWrapper(llm)), \n",
    "               FactualCorrectness(llm=LangchainLLMWrapper(llm)), \n",
    "               Faithfulness(llm=LangchainLLMWrapper(llm)), \n",
    "               SemanticSimilarity(embeddings=LangchainEmbeddingsWrapper(OpenAIEmbeddings()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ee9978b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_query': 'Who is Carolyn Matheus?', 'LLMContextRecall': 0.0, 'FactualCorrectness': 0.0, 'Faithfulness': 1.0, 'SemanticSimilarity': 0.9328844088386496}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate our pipeline responses\n",
    "evalResults = await pipelineEvaluation(evaluationSamples, evalMetrics)\n",
    "for result in evalResults:\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM-LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
