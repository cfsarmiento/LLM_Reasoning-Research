{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Corrective RAG Implementation\n",
    "Author: Christian Sarmiento\n",
    "Purpose: This notebook is intended to get a Self-RAG implementation set up with LangChain/LangGraph.\n",
    "Date Created: 12/4/24\n",
    "Last Updated: 12/4/24\n",
    "Data: Marist College Administrative Corpus Dataset\n",
    "Sources:\n",
    "- https://blog.langchain.dev/agentic-rag-with-langgraph/\n",
    "- https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_crag.ipynb?ref=blog.langchain.dev\n",
    "Note: Most of the code for graph implementation of CRAG was taken from the second source.\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "RAG Research             |               Machine Learning Independent Study             |              DR. EITEL LAURIA\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tavily-python\n",
      "  Downloading tavily_python-0.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: requests in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from tavily-python) (2.32.3)\n",
      "Requirement already satisfied: tiktoken>=0.5.1 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from tavily-python) (0.7.0)\n",
      "Requirement already satisfied: httpx in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from tavily-python) (0.27.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from tiktoken>=0.5.1->tavily-python) (2024.9.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from requests->tavily-python) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from requests->tavily-python) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from requests->tavily-python) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from requests->tavily-python) (2024.8.30)\n",
      "Requirement already satisfied: anyio in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from httpx->tavily-python) (4.6.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from httpx->tavily-python) (1.0.5)\n",
      "Requirement already satisfied: sniffio in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from httpx->tavily-python) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from httpcore==1.*->httpx->tavily-python) (0.14.0)\n",
      "Downloading tavily_python-0.5.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: tavily-python\n",
      "Successfully installed tavily-python-0.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tavily-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "sys.path.append(\"/Users/christiansarmiento/Library/CloudStorage/OneDrive-MaristCollege/Machine Learning/Private Code\")\n",
    "from api_keys import openAIKey\n",
    "from api_keys import langchainKey\n",
    "from api_keys import tavilyKey  # web search packages\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema import Document\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from typing import List, Dict, Optional\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import gradio as gr  # easy frontend implementation\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, SemanticSimilarity\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas import SingleTurnSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain Enviornment Variables\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = langchainKey()\n",
    "os.environ[\"OPENAI_API_KEY\"] = openAIKey()\n",
    "os.environ[\"TAVILY_API_KEY\"] = tavilyKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "csvPath = \"/Users/christiansarmiento/Library/CloudStorage/OneDrive-MaristCollege/Machine Learning/Data/Marist_QA.csv\"\n",
    "maristQA = pd.read_csv(csvPath, header=None)\n",
    "\n",
    "# To use RecursiveCharacterTextSplitter, we need a list of dictionaries\n",
    "maristContext = [Document(page_content=text) for text in maristQA[1].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Documents into Chunks\n",
    "textSplitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, add_start_index=True)\n",
    "texts = textSplitter.split_documents(maristContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store Documents in Vector DB (Chroma)\n",
    "vectorDB = Chroma.from_documents(documents=texts, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# Setup Retrieval System\n",
    "retriever = vectorDB.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})  # Retrieves 3 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='no'\n"
     ]
    }
   ],
   "source": [
    "# Define Grader class for document grading in CRAG\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "# LLM with function call\n",
    "llmDocGrader = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "structuredLLMGrader = llmDocGrader.with_structured_output(GradeDocuments)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "   tic meaning related to the user question, grade it as relevant. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "gradePrompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Grader Chain\n",
    "retrievalGrader = gradePrompt | structuredLLMGrader\n",
    "\n",
    "# Testing it\n",
    "question = \"agent memory\"\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "docTxt = docs[1].page_content\n",
    "print(retrievalGrader.invoke({\"question\": question, \"document\": docTxt}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "# Generation Chain\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "ragChain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "generation = ragChain.invoke({\"context\": docs, \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are the key concepts and techniques related to agent memory in artificial intelligence?'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question Rewriter\n",
    "llmRewriter = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n \n",
    "     for vectorstore retrieval. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\n",
    "rewritePrompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "questionRewriter = rewritePrompt | llm | StrOutputParser()\n",
    "questionRewriter.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web Search Component\n",
    "webSearchTool = TavilySearchResults(k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metric evaluator\n",
    "\n",
    "## Evaluation LLM & embeddings\n",
    "evalLLM = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\", temperature=0))\n",
    "evalEmbeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())\n",
    "\n",
    "## Initialize metrics with LLM and embeddings\n",
    "contextRecall = LLMContextRecall(llm=evalLLM)\n",
    "faithfulness = Faithfulness(llm=evalLLM)\n",
    "factualCorrectness = FactualCorrectness(llm=evalLLM)\n",
    "semanticSimilarity = SemanticSimilarity(embeddings=evalEmbeddings)\n",
    "\n",
    "## Collect metrics\n",
    "evalMetrics = [\n",
    "    contextRecall,\n",
    "    faithfulness,\n",
    "    factualCorrectness,\n",
    "    semanticSimilarity\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Graph structure for Self-RAG\n",
    "\n",
    "# Graph State\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        documents: list of documents\n",
    "        web_search: whether to add search\n",
    "        metrics: evaluation metrics for each generation \n",
    "    \"\"\"\n",
    "    question: str\n",
    "    generation: str\n",
    "    webSearch: str\n",
    "    documents: List[str]\n",
    "    metrics: Optional[Dict[str, float]]\n",
    "\n",
    "## Nodes\n",
    "\n",
    "# Retrieval Node     \n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.get_relevant_documents(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "# Generation Node\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # RAG generation\n",
    "    generation = ragChain.invoke({\"context\": documents, \"question\": question})\n",
    "\n",
    "    # Return updated state\n",
    "    updatedState = {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "    return updatedState\n",
    "\n",
    "# Grader Node\n",
    "def gradeDocuments(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with only filtered relevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filteredDocs = []\n",
    "    webSearch = \"No\"\n",
    "    for d in documents:\n",
    "\n",
    "        score = retrievalGrader.invoke(\n",
    "            {\"question\": question, \"document\": d.page_content}\n",
    "        )\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filteredDocs.append(d)\n",
    "\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            webSearch = \"Yes\"\n",
    "            continue\n",
    "\n",
    "    return {\"documents\": filteredDocs, \"question\": question, \"webSearch\": webSearch}\n",
    "\n",
    "\n",
    "# Rewriter node\n",
    "def transformQuery(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates question key with a re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Re-write question\n",
    "    betterQuestion = questionRewriter.invoke({\"question\": question})\n",
    "    return {\"documents\": documents, \"question\": betterQuestion}\n",
    "\n",
    "def webSearch(state):\n",
    "    \"\"\"\n",
    "    Web search based on the re-phrased question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with appended web results\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Web search\n",
    "    docs = webSearchTool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    documents.append(web_results)\n",
    "\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "## Edges\n",
    "\n",
    "# Generation edge\n",
    "def decideToGenerate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or re-generate a question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    state[\"question\"]\n",
    "    webSearch = state[\"webSearch\"]\n",
    "    state[\"documents\"]\n",
    "\n",
    "    if webSearch == \"Yes\":\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\n",
    "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\"\n",
    "        )\n",
    "        return \"transformQuery\"\n",
    "    \n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "    \n",
    "async def evaluateMetrics(state):\n",
    "    \"\"\"\n",
    "    Evaluate metrics for the current RAG pipeline response.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state.\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Adds a 'metrics' key containing evaluation scores.\n",
    "    \"\"\"\n",
    "    print(\"---EVALUATING METRICS---\")\n",
    "    question = state[\"question\"]\n",
    "    generation = state[\"generation\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Mock ground truth if unavailable (replace with actual reference if possible)\n",
    "    groundTruth = state.get(\"groundTruth\", \"Expected answer based on context.\")\n",
    "\n",
    "    # Prepare retrieved contexts\n",
    "    retrievedContexts = [doc.page_content for doc in documents]\n",
    "\n",
    "    # Create a SingleTurnSample object\n",
    "    sample = SingleTurnSample(\n",
    "        user_input=question,\n",
    "        response=generation,\n",
    "        reference=groundTruth,\n",
    "        retrieved_contexts=retrievedContexts,\n",
    "    )\n",
    "\n",
    "    # Evaluate metrics\n",
    "    state[\"metrics\"] = {\n",
    "        \"LLMContextRecall\": await contextRecall.single_turn_ascore(sample),\n",
    "        \"Faithfulness\": await faithfulness.single_turn_ascore(sample),\n",
    "        \"FactualCorrectness\": await factualCorrectness.single_turn_ascore(sample),\n",
    "        \"SemanticSimilarity\": await semanticSimilarity.single_turn_ascore(sample),\n",
    "    }\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"retrieve\", retrieve)  \n",
    "workflow.add_node(\"gradeDocuments\", gradeDocuments)  \n",
    "workflow.add_node(\"generate\", generate)  \n",
    "workflow.add_node(\"transformQuery\", transformQuery)  \n",
    "workflow.add_node(\"webSearchNode\", webSearch)\n",
    "workflow.add_node(\"evaluateMetrics\", evaluateMetrics)  \n",
    "\n",
    "# Build graph\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"gradeDocuments\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"gradeDocuments\",\n",
    "    decideToGenerate,\n",
    "    {\n",
    "        \"transformQuery\": \"transformQuery\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"transformQuery\", \"webSearchNode\")\n",
    "workflow.add_edge(\"webSearchNode\", \"generate\")\n",
    "workflow.add_edge(\"generate\", \"evaluateMetrics\")\n",
    "workflow.add_edge(\"evaluateMetrics\", END)\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "\"Node 'retrieve':\"\n",
      "'\\n---\\n'\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "\"Node 'gradeDocuments':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "---EVALUATING METRICS---\n",
      "\"Node 'evaluateMetrics':\"\n",
      "'Metrics: '\n",
      "{'FactualCorrectness': 0.0,\n",
      " 'Faithfulness': 1.0,\n",
      " 'LLMContextRecall': 0.0,\n",
      " 'SemanticSimilarity': 0.7259663840810067}\n",
      "'\\n---\\n'\n",
      "'Final Generation: '\n",
      "('Dr. Carolyn C. Matheus is an Associate Professor of Information Systems and '\n",
      " 'the Director of the Honors Program at Marist College. She holds a PhD in '\n",
      " 'Organizational Studies with a focus on leadership from SUNY Albany and has '\n",
      " 'received the National Society of Leadership and Success award for Excellence '\n",
      " 'in Teaching. Dr. Matheus is involved in faculty-student research projects '\n",
      " 'and offers seminars on authentic leadership and innovation.')\n",
      "'Final Metrics: '\n",
      "{'FactualCorrectness': 0.0,\n",
      " 'Faithfulness': 1.0,\n",
      " 'LLMContextRecall': 0.0,\n",
      " 'SemanticSimilarity': 0.7259663840810067}\n"
     ]
    }
   ],
   "source": [
    "# Inital test run\n",
    "inputs = {\"question\": \"Who is Carolyn Matheus?\"}\n",
    "async for output in app.astream(inputs):\n",
    "    for key, value in output.items():\n",
    "\n",
    "        # Print node\n",
    "        pprint(f\"Node '{key}':\")\n",
    "\n",
    "        # Optional: print full state at each node\n",
    "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "\n",
    "        # Print metrics\n",
    "        if \"metrics\" in value:\n",
    "            pprint(\"Metrics: \")\n",
    "            pprint(value[\"metrics\"])\n",
    "\n",
    "    pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint(\"Final Generation: \")\n",
    "pprint(value[\"generation\"])\n",
    "\n",
    "# Final metrics\n",
    "if \"metrics\" in value:\n",
    "    pprint(\"Final Metrics: \")\n",
    "    pprint(value[\"metrics\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put CRAG into Gradio\n",
    "evaluationSamples = []\n",
    "async def correctiveRAG(userQuery, history, correctAnswer=None):\n",
    "    \"\"\"\n",
    "    Gradio-compatible function to process CRAG workflow.\n",
    "    Args:\n",
    "        userQuery (str): The user's question.\n",
    "        history (list): Conversation history.\n",
    "        correctAnswer (str): The ground truth answer for metrics (optional).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (chatDisplay, history)\n",
    "    \"\"\"\n",
    "\n",
    "    # Get our input together\n",
    "    inputs = {\"question\": userQuery}\n",
    "\n",
    "    # Start the workflow\n",
    "    finalOutput = None\n",
    "    async for output in app.astream(inputs):\n",
    "\n",
    "        # Saving final output for metric purposes\n",
    "        finalOutput = output\n",
    "\n",
    "        # Printing out each node state for clarity\n",
    "        for key, value in output.items():\n",
    "\n",
    "            # Print node\n",
    "            pprint(f\"Node '{key}':\")\n",
    "\n",
    "            # Print metrics\n",
    "            if \"metrics\" in value:\n",
    "                pprint(\"Metrics: \")\n",
    "                pprint(value[\"metrics\"])\n",
    "    \n",
    "    # Get the generation and its metrics\n",
    "    finalNodeKey = list(finalOutput.keys())[-1]  # Get the key of the last executed node\n",
    "    nodeOutput = finalOutput[finalNodeKey]  # Access the nested state\n",
    "    generation = nodeOutput.get(\"generation\", \"No generation produced.\")\n",
    "    metrics = nodeOutput.get(\"metrics\", {})\n",
    "\n",
    "    # Update history\n",
    "    if history is None:\n",
    "        history = []\n",
    "    \n",
    "    history.extend([\n",
    "        {\"role\": \"user\", \"content\": userQuery},\n",
    "        {\"role\": \"llm\", \"content\": generation}\n",
    "    ])\n",
    "\n",
    "    # Display output for gradio\n",
    "    chatDisplay = [(msg[\"content\"], \"User\" if msg[\"role\"] == \"user\" else \"LLM\") for msg in history]\n",
    "\n",
    "    # Append metrics to evaluationSamples for tracking (if correctAnswer is provided)\n",
    "    if correctAnswer:\n",
    "        evaluationSamples.append({\n",
    "            \"user_input\": userQuery,\n",
    "            \"retrieved_contexts\": [doc.page_content for doc in finalOutput.get(\"documents\", [])],\n",
    "            \"response\": generation,\n",
    "            \"reference\": correctAnswer,\n",
    "            \"metrics\": metrics,\n",
    "        })\n",
    "\n",
    "    return history #, chatDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages/gradio/components/chatbot.py:222: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "\"Node 'retrieve':\"\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "\"Node 'gradeDocuments':\"\n",
      "---GENERATE---\n",
      "\"Node 'generate':\"\n",
      "---EVALUATING METRICS---\n",
      "\"Node 'evaluateMetrics':\"\n",
      "'Metrics: '\n",
      "{'FactualCorrectness': 0.0,\n",
      " 'Faithfulness': 1.0,\n",
      " 'LLMContextRecall': 0.0,\n",
      " 'SemanticSimilarity': 0.7259663840810067}\n"
     ]
    }
   ],
   "source": [
    "# Gradio frontend\n",
    "interface = gr.Interface(\n",
    "    fn=correctiveRAG,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Ask a Question\", placeholder=\"Enter your question here...\"),\n",
    "        gr.State(),  # Keeps track of conversation history\n",
    "        gr.Textbox(label=\"Correct Answer (Optional)\", placeholder=\"For evaluation purposes...\"),\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Chatbot(label=\"CorrectiveRAG Conversation\"),\n",
    "        gr.State(),  # Updates conversation history\n",
    "    ],\n",
    "    title=\"CorrectiveRAG Implementation\",\n",
    "    description=\"Interact with the CRAG workflow for document-grounded question answering.\",\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate our RAG pipeline when given ground truth\n",
    "async def pipelineEvaluation(dataset, metrics):\n",
    "\n",
    "    # Run through our runs\n",
    "    results = []\n",
    "    for run in dataset:\n",
    "\n",
    "        # Save our inputs/outputs\n",
    "        inputQuery = run[\"user_input\"]\n",
    "        groundTruthAnswer = run[\"reference\"]\n",
    "        contexts = run[\"retrieved_contexts\"]\n",
    "        response = run[\"response\"]\n",
    "\n",
    "        # Create a SingleTurnSample object\n",
    "        sample = SingleTurnSample(\n",
    "            user_input=inputQuery,\n",
    "            response=response,\n",
    "            reference=groundTruthAnswer,\n",
    "            retrieved_contexts=contexts \n",
    "        )\n",
    "\n",
    "        # Evaluate metrics\n",
    "        runResults = {\"input_query\": inputQuery}\n",
    "        for metric in metrics:\n",
    "\n",
    "            # Get the score for the given metric\n",
    "            try:\n",
    "\n",
    "                score = await metric.single_turn_ascore(sample)\n",
    "                runResults[type(metric).__name__] = score\n",
    "\n",
    "            except Exception as e:\n",
    "                # Catch errors for debugging\n",
    "                runResults[type(metric).__name__] = f\"Error: {str(e)}\"\n",
    "        \n",
    "        # Save metric results\n",
    "        results.append(runResults)\n",
    "    \n",
    "    # Calculate mean and standard deviation for each metric\n",
    "    metricsStats = {}\n",
    "    for metric in metrics:\n",
    "        metricName = type(metric).__name__\n",
    "        scores = [result[metricName] for result in results if isinstance(result[metricName], (int, float))]\n",
    "        \n",
    "        # Only calculate stats if there are valid scores\n",
    "        if scores:\n",
    "            metricsStats[metricName] = {\n",
    "                \"mean\": np.mean(scores),\n",
    "                \"std_dev\": np.std(scores),\n",
    "            }\n",
    "            \n",
    "        else:\n",
    "            metricsStats[metricName] = {\n",
    "                \"mean\": \"No valid scores\",\n",
    "                \"std_dev\": \"No valid scores\",\n",
    "            }\n",
    "    \n",
    "    return results, metricsStats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metrics\n",
    "evalMetrics = [LLMContextRecall(llm=LangchainLLMWrapper(llm)), \n",
    "               FactualCorrectness(llm=LangchainLLMWrapper(llm)), \n",
    "               Faithfulness(llm=LangchainLLMWrapper(llm)), \n",
    "               SemanticSimilarity(embeddings=LangchainEmbeddingsWrapper(OpenAIEmbeddings()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_query': 'Who is Carolyn Matheus?', 'LLMContextRecall': 0.0, 'FactualCorrectness': 0.0, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.928909234587568}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate our pipeline responses\n",
    "evalResults = await pipelineEvaluation(evaluationSamples, evalMetrics)\n",
    "for result in evalResults:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>How many transfer students does Marist have?</td>\n",
       "      <td>\"Transfer AdmissionTransfer Students Thrive at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>What classes do you need for the gender studie...</td>\n",
       "      <td>\"Women's, Gender, and Sexuality StudiesWomen's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>Who is Gregory Machacek?</td>\n",
       "      <td>\"Contact InformationAcademic SchoolOfficeEmail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>Who is Mark James Morreal?</td>\n",
       "      <td>\"Contact InformationAcademic SchoolOfficeEmail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>School psychology success rate</td>\n",
       "      <td>\"Master of Arts in School Psychology Departmen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0  \\\n",
       "639       How many transfer students does Marist have?   \n",
       "65   What classes do you need for the gender studie...   \n",
       "485                           Who is Gregory Machacek?   \n",
       "516                         Who is Mark James Morreal?   \n",
       "89                      School psychology success rate   \n",
       "\n",
       "                                                     1  \n",
       "639  \"Transfer AdmissionTransfer Students Thrive at...  \n",
       "65   \"Women's, Gender, and Sexuality StudiesWomen's...  \n",
       "485  \"Contact InformationAcademic SchoolOfficeEmail...  \n",
       "516  \"Contact InformationAcademic SchoolOfficeEmail...  \n",
       "89   \"Master of Arts in School Psychology Departmen...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample 222 records from our dataset\n",
    "maristTestSample = maristQA.sample(10, replace=False)\n",
    "maristTestSample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6g/8868nrgn5znbjrhdbycnn8pw0000gn/T/ipykernel_82448/1132511695.py:2: RuntimeWarning: coroutine 'correctiveRAG' was never awaited\n",
      "  chatHistory = None\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "\"Node 'retrieve':\"\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "\"Node 'gradeDocuments':\"\n",
      "---GENERATE---\n",
      "\"Node 'generate':\"\n",
      "---EVALUATING METRICS---\n",
      "\"Node 'evaluateMetrics':\"\n",
      "'Metrics: '\n",
      "{'FactualCorrectness': 0.0,\n",
      " 'Faithfulness': 1.0,\n",
      " 'LLMContextRecall': 0.0,\n",
      " 'SemanticSimilarity': 0.724199651421554}\n",
      "---RETRIEVE---\n",
      "\"Node 'retrieve':\"\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\n",
      "\"Node 'gradeDocuments':\"\n",
      "---TRANSFORM QUERY---\n",
      "\"Node 'transformQuery':\"\n",
      "---WEB SEARCH---\n",
      "\"Node 'webSearchNode':\"\n",
      "---GENERATE---\n",
      "\"Node 'generate':\"\n",
      "---EVALUATING METRICS---\n",
      "\"Node 'evaluateMetrics':\"\n",
      "'Metrics: '\n",
      "{'FactualCorrectness': 0.0,\n",
      " 'Faithfulness': 1.0,\n",
      " 'LLMContextRecall': 0.0,\n",
      " 'SemanticSimilarity': 0.7249696060087707}\n",
      "---RETRIEVE---\n",
      "\"Node 'retrieve':\"\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\n",
      "\"Node 'gradeDocuments':\"\n",
      "---TRANSFORM QUERY---\n",
      "\"Node 'transformQuery':\"\n",
      "---WEB SEARCH---\n",
      "\"Node 'webSearchNode':\"\n",
      "---GENERATE---\n",
      "\"Node 'generate':\"\n",
      "---EVALUATING METRICS---\n",
      "\"Node 'evaluateMetrics':\"\n",
      "'Metrics: '\n",
      "{'FactualCorrectness': 0.0,\n",
      " 'Faithfulness': 1.0,\n",
      " 'LLMContextRecall': 0.0,\n",
      " 'SemanticSimilarity': 0.7357408038688673}\n",
      "---RETRIEVE---\n",
      "\"Node 'retrieve':\"\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\n",
      "\"Node 'gradeDocuments':\"\n",
      "---TRANSFORM QUERY---\n",
      "\"Node 'transformQuery':\"\n",
      "---WEB SEARCH---\n",
      "\"Node 'webSearchNode':\"\n",
      "---GENERATE---\n",
      "\"Node 'generate':\"\n",
      "---EVALUATING METRICS---\n",
      "\"Node 'evaluateMetrics':\"\n",
      "'Metrics: '\n",
      "{'FactualCorrectness': 0.0,\n",
      " 'Faithfulness': 1.0,\n",
      " 'LLMContextRecall': 0.0,\n",
      " 'SemanticSimilarity': 0.7357964311434045}\n",
      "---RETRIEVE---\n",
      "\"Node 'retrieve':\"\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\n",
      "\"Node 'gradeDocuments':\"\n",
      "---TRANSFORM QUERY---\n",
      "\"Node 'transformQuery':\"\n",
      "---WEB SEARCH---\n",
      "\"Node 'webSearchNode':\"\n",
      "---GENERATE---\n",
      "\"Node 'generate':\"\n",
      "---EVALUATING METRICS---\n",
      "\"Node 'evaluateMetrics':\"\n",
      "'Metrics: '\n",
      "{'FactualCorrectness': 0.33,\n",
      " 'Faithfulness': 1.0,\n",
      " 'LLMContextRecall': 0.0,\n",
      " 'SemanticSimilarity': 0.7764098561586668}\n",
      "---RETRIEVE---\n",
      "\"Node 'retrieve':\"\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "\"Node 'gradeDocuments':\"\n",
      "---GENERATE---\n",
      "\"Node 'generate':\"\n",
      "---EVALUATING METRICS---\n",
      "\"Node 'evaluateMetrics':\"\n",
      "'Metrics: '\n",
      "{'FactualCorrectness': 0.0,\n",
      " 'Faithfulness': 0.6666666666666666,\n",
      " 'LLMContextRecall': 0.0,\n",
      " 'SemanticSimilarity': 0.7195379208210739}\n",
      "---RETRIEVE---\n",
      "\"Node 'retrieve':\"\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "\"Node 'gradeDocuments':\"\n",
      "---GENERATE---\n",
      "\"Node 'generate':\"\n",
      "---EVALUATING METRICS---\n",
      "\"Node 'evaluateMetrics':\"\n",
      "'Metrics: '\n",
      "{'FactualCorrectness': 0.18,\n",
      " 'Faithfulness': 1.0,\n",
      " 'LLMContextRecall': 0.0,\n",
      " 'SemanticSimilarity': 0.7357857090561359}\n",
      "---RETRIEVE---\n",
      "\"Node 'retrieve':\"\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "\"Node 'gradeDocuments':\"\n",
      "---GENERATE---\n",
      "\"Node 'generate':\"\n",
      "---EVALUATING METRICS---\n",
      "\"Node 'evaluateMetrics':\"\n",
      "'Metrics: '\n",
      "{'FactualCorrectness': 0.0,\n",
      " 'Faithfulness': 1.0,\n",
      " 'LLMContextRecall': 0.0,\n",
      " 'SemanticSimilarity': 0.7170270802017041}\n",
      "---RETRIEVE---\n",
      "\"Node 'retrieve':\"\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "\"Node 'gradeDocuments':\"\n",
      "---GENERATE---\n",
      "\"Node 'generate':\"\n",
      "---EVALUATING METRICS---\n",
      "\"Node 'evaluateMetrics':\"\n",
      "'Metrics: '\n",
      "{'FactualCorrectness': 0.0,\n",
      " 'Faithfulness': 1.0,\n",
      " 'LLMContextRecall': 0.0,\n",
      " 'SemanticSimilarity': 0.7282003257603136}\n",
      "---RETRIEVE---\n",
      "\"Node 'retrieve':\"\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "\"Node 'gradeDocuments':\"\n",
      "---GENERATE---\n",
      "\"Node 'generate':\"\n",
      "---EVALUATING METRICS---\n",
      "\"Node 'evaluateMetrics':\"\n",
      "'Metrics: '\n",
      "{'FactualCorrectness': 0.67,\n",
      " 'Faithfulness': 1.0,\n",
      " 'LLMContextRecall': 0.0,\n",
      " 'SemanticSimilarity': 0.7367293033435603}\n",
      "{'input_query': 'How many transfer students does Marist have?', 'LLMContextRecall': 0.6, 'Faithfulness': 0.0, 'FactualCorrectness': 0.33, 'SemanticSimilarity': 0.9249107885097787}\n",
      "{'input_query': 'What classes do you need for the gender studies pathway?', 'LLMContextRecall': 1.0, 'Faithfulness': 0.0, 'FactualCorrectness': 0.0, 'SemanticSimilarity': 0.8996125110103931}\n",
      "{'input_query': 'Who is Gregory Machacek?', 'LLMContextRecall': 0.0, 'Faithfulness': 0.0, 'FactualCorrectness': 0.27, 'SemanticSimilarity': 0.9296499569368427}\n",
      "{'input_query': 'Who is Mark James Morreal?', 'LLMContextRecall': 0.0, 'Faithfulness': 0.0, 'FactualCorrectness': 0.41, 'SemanticSimilarity': 0.9006930475722923}\n",
      "{'input_query': ' School psychology success rate', 'LLMContextRecall': 0.0, 'Faithfulness': 0.5, 'FactualCorrectness': 0.25, 'SemanticSimilarity': 0.7993874588153614}\n",
      "{'input_query': 'Benefits of doing internships?', 'LLMContextRecall': 0.0, 'Faithfulness': 0.0, 'FactualCorrectness': 0.3, 'SemanticSimilarity': 0.7891078314825829}\n",
      "{'input_query': 'Tell me about the president search.', 'LLMContextRecall': 0.0, 'Faithfulness': 0.0, 'FactualCorrectness': 0.49, 'SemanticSimilarity': 0.9274124538609068}\n",
      "{'input_query': 'How is the Master of Science in Information Systems?', 'LLMContextRecall': 0.0, 'Faithfulness': 0.0, 'FactualCorrectness': 0.43, 'SemanticSimilarity': 0.9451138749780377}\n",
      "{'input_query': 'Graduate Study in the UK', 'LLMContextRecall': 0.45454545454545453, 'Faithfulness': 0.0, 'FactualCorrectness': 0.23, 'SemanticSimilarity': 0.7024486784223513}\n",
      "{'input_query': \"What are the college's strategy/goals for the future?\", 'LLMContextRecall': 0.8181818181818182, 'Faithfulness': 0.0, 'FactualCorrectness': 0.45, 'SemanticSimilarity': 0.8894856344804422}\n",
      "LLMContextRecall - Mean: 0.2872727272727273, St. Dev: 0.37560309630035427\n",
      "Faithfulness - Mean: 0.05, St. Dev: 0.15000000000000002\n",
      "FactualCorrectness - Mean: 0.316, St. Dev: 0.13573503600765718\n",
      "SemanticSimilarity - Mean: 0.870782223606899, St. Dev: 0.07570155254901194\n"
     ]
    }
   ],
   "source": [
    "# Run our chain with each question and evaluate\n",
    "chatHistory = None\n",
    "for row in maristTestSample.iterrows():\n",
    "    chatHistory = await correctiveRAG(row[1][0], chatHistory, row[1][1])\n",
    "\n",
    "## Evaluation\n",
    "evalResults, metricStats = await pipelineEvaluation(evaluationSamples, evalMetrics)\n",
    "for result in evalResults:\n",
    "    print(result)\n",
    "\n",
    "for metric in metricStats.keys():\n",
    "    print(f\"{metric} - Mean: {metricStats[metric]['mean']}, St. Dev: {metricStats[metric]['std_dev']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM-LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
