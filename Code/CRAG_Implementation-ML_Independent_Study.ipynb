{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Corrective RAG Implementation\n",
    "Author: Christian Sarmiento\n",
    "Purpose: This notebook is intended to get a Self-RAG implementation set up with LangChain/LangGraph.\n",
    "Date Created: 12/4/24\n",
    "Last Updated: 12/4/24\n",
    "Data: Marist College Administrative Corpus Dataset\n",
    "Sources:\n",
    "- https://blog.langchain.dev/agentic-rag-with-langgraph/\n",
    "- https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_self_rag.ipynb?ref=blog.langchain.dev\n",
    "- ChatGPT: o1-preview\n",
    "Note: Most of the code for graph implementation of Self-RAG was taken from the second source.\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "RAG Research             |               Machine Learning Independent Study             |              DR. EITEL LAURIA\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tavily-python\n",
      "  Downloading tavily_python-0.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: requests in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from tavily-python) (2.32.3)\n",
      "Requirement already satisfied: tiktoken>=0.5.1 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from tavily-python) (0.7.0)\n",
      "Requirement already satisfied: httpx in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from tavily-python) (0.27.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from tiktoken>=0.5.1->tavily-python) (2024.9.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from requests->tavily-python) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from requests->tavily-python) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from requests->tavily-python) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from requests->tavily-python) (2024.8.30)\n",
      "Requirement already satisfied: anyio in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from httpx->tavily-python) (4.6.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from httpx->tavily-python) (1.0.5)\n",
      "Requirement already satisfied: sniffio in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from httpx->tavily-python) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from httpcore==1.*->httpx->tavily-python) (0.14.0)\n",
      "Downloading tavily_python-0.5.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: tavily-python\n",
      "Successfully installed tavily-python-0.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tavily-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3577: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in _VertexAIBase has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in _VertexAICommon has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import sys\n",
    "sys.path.append(\"/Users/christiansarmiento/Library/CloudStorage/OneDrive-MaristCollege/Machine Learning/Private Code\")\n",
    "from api_keys import openAIKey\n",
    "from api_keys import langchainKey\n",
    "from api_keys import tavilyKey  # web search packages\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema import Document\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from typing import List, Dict, Optional\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import gradio as gr  # easy frontend implementation\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, SemanticSimilarity\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas import SingleTurnSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain Enviornment Variables\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = langchainKey()\n",
    "os.environ[\"OPENAI_API_KEY\"] = openAIKey()\n",
    "os.environ[\"TAVILY_API_KEY\"] = tavilyKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "csvPath = \"/Users/christiansarmiento/Library/CloudStorage/OneDrive-MaristCollege/Machine Learning/Data/Marist_QA.csv\"\n",
    "maristQA = pd.read_csv(csvPath, header=None)\n",
    "\n",
    "# To use RecursiveCharacterTextSplitter, we need a list of dictionaries\n",
    "maristContext = [Document(page_content=text) for text in maristQA[1].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Documents into Chunks\n",
    "textSplitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, add_start_index=True)\n",
    "texts = textSplitter.split_documents(maristContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store Documents in Vector DB (Chroma)\n",
    "vectorDB = Chroma.from_documents(documents=texts, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# Setup Retrieval System\n",
    "retriever = vectorDB.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})  # Retrieves 3 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6g/8868nrgn5znbjrhdbycnn8pw0000gn/T/ipykernel_82448/2934397582.py:30: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='no'\n"
     ]
    }
   ],
   "source": [
    "# Define Grader class for document grading in CRAG\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "# LLM with function call\n",
    "llmDocGrader = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "structuredLLMGrader = llmDocGrader.with_structured_output(GradeDocuments)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "   tic meaning related to the user question, grade it as relevant. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "gradePrompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Grader Chain\n",
    "retrievalGrader = gradePrompt | structuredLLMGrader\n",
    "\n",
    "# Testing it\n",
    "question = \"agent memory\"\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "docTxt = docs[1].page_content\n",
    "print(retrievalGrader.invoke({\"question\": question, \"document\": docTxt}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "# Generation Chain\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "ragChain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "generation = ragChain.invoke({\"context\": docs, \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are the key concepts and functionalities of agent memory in artificial intelligence?'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question Rewriter\n",
    "llmRewriter = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n \n",
    "     for vectorstore retrieval. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\n",
    "rewritePrompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "questionRewriter = rewritePrompt | llm | StrOutputParser()\n",
    "questionRewriter.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web Search Component\n",
    "webSearchTool = TavilySearchResults(k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metric evaluator\n",
    "\n",
    "## Evaluation LLM & embeddings\n",
    "evalLLM = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\", temperature=0))\n",
    "evalEmbeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())\n",
    "\n",
    "## Initialize metrics with LLM and embeddings\n",
    "contextRecall = LLMContextRecall(llm=evalLLM)\n",
    "faithfulness = Faithfulness(llm=evalLLM)\n",
    "factualCorrectness = FactualCorrectness(llm=evalLLM)\n",
    "semanticSimilarity = SemanticSimilarity(embeddings=evalEmbeddings)\n",
    "\n",
    "## Collect metrics\n",
    "evalMetrics = [\n",
    "    contextRecall,\n",
    "    faithfulness,\n",
    "    factualCorrectness,\n",
    "    semanticSimilarity\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Graph structure for Self-RAG\n",
    "\n",
    "# Graph State\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        documents: list of documents\n",
    "        web_search: whether to add search\n",
    "        metrics: evaluation metrics for each generation \n",
    "    \"\"\"\n",
    "    question: str\n",
    "    generation: str\n",
    "    webSearch: str\n",
    "    documents: List[str]\n",
    "    metrics: Optional[Dict[str, float]]\n",
    "\n",
    "## Nodes\n",
    "\n",
    "# Retrieval Node     \n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.get_relevant_documents(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "# Generation Node\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # RAG generation\n",
    "    generation = ragChain.invoke({\"context\": documents, \"question\": question})\n",
    "\n",
    "    # Return updated state\n",
    "    updatedState = {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "    return updatedState\n",
    "\n",
    "# Grader Node\n",
    "def gradeDocuments(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with only filtered relevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filteredDocs = []\n",
    "    webSearch = \"No\"\n",
    "    for d in documents:\n",
    "\n",
    "        score = retrievalGrader.invoke(\n",
    "            {\"question\": question, \"document\": d.page_content}\n",
    "        )\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filteredDocs.append(d)\n",
    "\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            webSearch = \"Yes\"\n",
    "            continue\n",
    "\n",
    "    return {\"documents\": filteredDocs, \"question\": question, \"webSearch\": webSearch}\n",
    "\n",
    "\n",
    "# Rewriter node\n",
    "def transformQuery(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates question key with a re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Re-write question\n",
    "    betterQuestion = questionRewriter.invoke({\"question\": question})\n",
    "    return {\"documents\": documents, \"question\": betterQuestion}\n",
    "\n",
    "def webSearch(state):\n",
    "    \"\"\"\n",
    "    Web search based on the re-phrased question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with appended web results\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Web search\n",
    "    docs = webSearchTool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    documents.append(web_results)\n",
    "\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "## Edges\n",
    "\n",
    "# Generation edge\n",
    "def decideToGenerate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or re-generate a question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    state[\"question\"]\n",
    "    webSearch = state[\"webSearch\"]\n",
    "    state[\"documents\"]\n",
    "\n",
    "    if webSearch == \"Yes\":\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\n",
    "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\"\n",
    "        )\n",
    "        return \"transformQuery\"\n",
    "    \n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "    \n",
    "async def evaluateMetrics(state):\n",
    "    \"\"\"\n",
    "    Evaluate metrics for the current RAG pipeline response.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state.\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Adds a 'metrics' key containing evaluation scores.\n",
    "    \"\"\"\n",
    "    print(\"---EVALUATING METRICS---\")\n",
    "    question = state[\"question\"]\n",
    "    generation = state[\"generation\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Mock ground truth if unavailable (replace with actual reference if possible)\n",
    "    groundTruth = state.get(\"groundTruth\", \"Expected answer based on context.\")\n",
    "\n",
    "    # Prepare retrieved contexts\n",
    "    retrievedContexts = [doc.page_content for doc in documents]\n",
    "\n",
    "    # Create a SingleTurnSample object\n",
    "    sample = SingleTurnSample(\n",
    "        user_input=question,\n",
    "        response=generation,\n",
    "        reference=groundTruth,\n",
    "        retrieved_contexts=retrievedContexts,\n",
    "    )\n",
    "\n",
    "    # Evaluate metrics\n",
    "    state[\"metrics\"] = {\n",
    "        \"LLMContextRecall\": await contextRecall.single_turn_ascore(sample),\n",
    "        \"Faithfulness\": await faithfulness.single_turn_ascore(sample),\n",
    "        \"FactualCorrectness\": await factualCorrectness.single_turn_ascore(sample),\n",
    "        \"SemanticSimilarity\": await semanticSimilarity.single_turn_ascore(sample),\n",
    "    }\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"retrieve\", retrieve)  \n",
    "workflow.add_node(\"gradeDocuments\", gradeDocuments)  \n",
    "workflow.add_node(\"generate\", generate)  \n",
    "workflow.add_node(\"transformQuery\", transformQuery)  \n",
    "workflow.add_node(\"webSearchNode\", webSearch)\n",
    "workflow.add_node(\"evaluateMetrics\", evaluateMetrics)  \n",
    "\n",
    "# Build graph\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"gradeDocuments\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"gradeDocuments\",\n",
    "    decideToGenerate,\n",
    "    {\n",
    "        \"transformQuery\": \"transformQuery\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"transformQuery\", \"webSearchNode\")\n",
    "workflow.add_edge(\"webSearchNode\", \"generate\")\n",
    "workflow.add_edge(\"generate\", \"evaluateMetrics\")\n",
    "workflow.add_edge(\"evaluateMetrics\", END)\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "\"Node 'retrieve':\"\n",
      "'\\n---\\n'\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "\"Node 'gradeDocuments':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "---EVALUATING METRICS---\n",
      "\"Node 'evaluateMetrics':\"\n",
      "'Metrics: '\n",
      "{'FactualCorrectness': 0.0,\n",
      " 'Faithfulness': 1.0,\n",
      " 'LLMContextRecall': 0.0,\n",
      " 'SemanticSimilarity': 0.7259663840810067}\n",
      "'\\n---\\n'\n",
      "'Final Generation: '\n",
      "('Dr. Carolyn C. Matheus is an Associate Professor of Information Systems and '\n",
      " 'the Director of the Honors Program at Marist College. She holds a PhD in '\n",
      " 'Organizational Studies with a focus on leadership from SUNY Albany and has '\n",
      " 'received the National Society of Leadership and Success award for Excellence '\n",
      " 'in Teaching. Dr. Matheus is involved in faculty-student research projects '\n",
      " 'and offers seminars on authentic leadership and innovation.')\n",
      "'Final Metrics: '\n",
      "{'FactualCorrectness': 0.0,\n",
      " 'Faithfulness': 1.0,\n",
      " 'LLMContextRecall': 0.0,\n",
      " 'SemanticSimilarity': 0.7259663840810067}\n"
     ]
    }
   ],
   "source": [
    "# Inital test run\n",
    "inputs = {\"question\": \"Who is Carolyn Matheus?\"}\n",
    "async for output in app.astream(inputs):\n",
    "    for key, value in output.items():\n",
    "\n",
    "        # Print node\n",
    "        pprint(f\"Node '{key}':\")\n",
    "\n",
    "        # Optional: print full state at each node\n",
    "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "\n",
    "        # Print metrics\n",
    "        if \"metrics\" in value:\n",
    "            pprint(\"Metrics: \")\n",
    "            pprint(value[\"metrics\"])\n",
    "\n",
    "    pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint(\"Final Generation: \")\n",
    "pprint(value[\"generation\"])\n",
    "\n",
    "# Final metrics\n",
    "if \"metrics\" in value:\n",
    "    pprint(\"Final Metrics: \")\n",
    "    pprint(value[\"metrics\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put CRAG into Gradio\n",
    "evaluationSamples = []\n",
    "async def selfRAG(userQuery, history, correctAnswer=None):\n",
    "    \"\"\"\n",
    "    Gradio-compatible function to process CRAG workflow.\n",
    "    Args:\n",
    "        userQuery (str): The user's question.\n",
    "        history (list): Conversation history.\n",
    "        correctAnswer (str): The ground truth answer for metrics (optional).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (chatDisplay, history)\n",
    "    \"\"\"\n",
    "\n",
    "    # Get our input together\n",
    "    inputs = {\"question\": userQuery}\n",
    "\n",
    "    # Start the workflow\n",
    "    finalOutput = None\n",
    "    async for output in app.astream(inputs):\n",
    "\n",
    "        # Saving final output for metric purposes\n",
    "        finalOutput = output\n",
    "\n",
    "        # Printing out each node state for clarity\n",
    "        for key, value in output.items():\n",
    "\n",
    "            # Print node\n",
    "            pprint(f\"Node '{key}':\")\n",
    "\n",
    "            # Print metrics\n",
    "            if \"metrics\" in value:\n",
    "                pprint(\"Metrics: \")\n",
    "                pprint(value[\"metrics\"])\n",
    "    \n",
    "    # Get the generation and its metrics\n",
    "    finalNodeKey = list(finalOutput.keys())[-1]  # Get the key of the last executed node\n",
    "    nodeOutput = finalOutput[finalNodeKey]  # Access the nested state\n",
    "    generation = nodeOutput.get(\"generation\", \"No generation produced.\")\n",
    "    metrics = nodeOutput.get(\"metrics\", {})\n",
    "\n",
    "    # Update history\n",
    "    if history is None:\n",
    "        history = []\n",
    "    \n",
    "    history.extend([\n",
    "        {\"role\": \"user\", \"content\": userQuery},\n",
    "        {\"role\": \"llm\", \"content\": generation}\n",
    "    ])\n",
    "\n",
    "    # Display output for gradio\n",
    "    chatDisplay = [(msg[\"content\"], \"User\" if msg[\"role\"] == \"user\" else \"LLM\") for msg in history]\n",
    "\n",
    "    # Append metrics to evaluationSamples for tracking (if correctAnswer is provided)\n",
    "    if correctAnswer:\n",
    "        evaluationSamples.append({\n",
    "            \"user_input\": userQuery,\n",
    "            \"retrieved_contexts\": [doc.page_content for doc in finalOutput.get(\"documents\", [])],\n",
    "            \"response\": generation,\n",
    "            \"reference\": correctAnswer,\n",
    "            \"metrics\": metrics,\n",
    "        })\n",
    "\n",
    "    return chatDisplay, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages/gradio/components/chatbot.py:222: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "\"Node 'retrieve':\"\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "\"Node 'gradeDocuments':\"\n",
      "---GENERATE---\n",
      "\"Node 'generate':\"\n",
      "---EVALUATING METRICS---\n",
      "\"Node 'evaluateMetrics':\"\n",
      "'Metrics: '\n",
      "{'FactualCorrectness': 0.0,\n",
      " 'Faithfulness': 1.0,\n",
      " 'LLMContextRecall': 0.0,\n",
      " 'SemanticSimilarity': 0.7259663840810067}\n"
     ]
    }
   ],
   "source": [
    "# Gradio frontend\n",
    "interface = gr.Interface(\n",
    "    fn=selfRAG,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Ask a Question\", placeholder=\"Enter your question here...\"),\n",
    "        gr.State(),  # Keeps track of conversation history\n",
    "        gr.Textbox(label=\"Correct Answer (Optional)\", placeholder=\"For evaluation purposes...\"),\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Chatbot(label=\"CorrectiveRAG Conversation\"),\n",
    "        gr.State(),  # Updates conversation history\n",
    "    ],\n",
    "    title=\"CorrectiveRAG Implementation\",\n",
    "    description=\"Interact with the CRAG workflow for document-grounded question answering.\",\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate our RAG pipeline when given ground truth\n",
    "async def pipelineEvaluation(dataset, metrics):\n",
    "\n",
    "    # Run through our runs\n",
    "    results = []\n",
    "    for run in dataset:\n",
    "\n",
    "        # Save our inputs/outputs\n",
    "        inputQuery = run[\"user_input\"]\n",
    "        groundTruthAnswer = run[\"reference\"]\n",
    "        contexts = run[\"retrieved_contexts\"]\n",
    "        response = run[\"response\"]\n",
    "\n",
    "        # Create a SingleTurnSample object\n",
    "        sample = SingleTurnSample(\n",
    "            user_input=inputQuery,\n",
    "            response=response,\n",
    "            reference=groundTruthAnswer,\n",
    "            retrieved_contexts=contexts \n",
    "        )\n",
    "\n",
    "        # Evaluate metrics\n",
    "        runResults = {\"input_query\": inputQuery}\n",
    "        for metric in metrics:\n",
    "\n",
    "            # Get the score for the given metric\n",
    "            try:\n",
    "\n",
    "                score = await metric.single_turn_ascore(sample)\n",
    "                runResults[type(metric).__name__] = score\n",
    "\n",
    "            except Exception as e:\n",
    "                # Catch errors for debugging\n",
    "                runResults[type(metric).__name__] = f\"Error: {str(e)}\"\n",
    "        \n",
    "        # Save metric results\n",
    "        results.append(runResults)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metrics\n",
    "evalMetrics = [LLMContextRecall(llm=LangchainLLMWrapper(llm)), \n",
    "               FactualCorrectness(llm=LangchainLLMWrapper(llm)), \n",
    "               Faithfulness(llm=LangchainLLMWrapper(llm)), \n",
    "               SemanticSimilarity(embeddings=LangchainEmbeddingsWrapper(OpenAIEmbeddings()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_query': 'Who is Carolyn Matheus?', 'LLMContextRecall': 0.0, 'FactualCorrectness': 0.0, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.928909234587568}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate our pipeline responses\n",
    "evalResults = await pipelineEvaluation(evaluationSamples, evalMetrics)\n",
    "for result in evalResults:\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM-LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
