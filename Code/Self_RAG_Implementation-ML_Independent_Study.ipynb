{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Self RAG Implementation\n",
    "Author: Christian Sarmiento\n",
    "Purpose: This notebook is intended to get a Self-RAG implementation set up with LangChain/LangGraph.\n",
    "Date Created: 11/17/24\n",
    "Last Updated: 11/29/24\n",
    "Data: https://archive.ics.uci.edu/dataset/450/sports+articles+for+objectivity+analysis\n",
    "Sources:\n",
    "- https://blog.langchain.dev/agentic-rag-with-langgraph/\n",
    "- https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_self_rag.ipynb?ref=blog.langchain.dev\n",
    "- ChatGPT: o1-preview\n",
    "Note: Most of the code for graph implementation of Self-RAG was taken from the second source.\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "RAG Research             |               Machine Learning Independent Study             |              DR. EITEL LAURIA\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langgraph\n",
      "  Downloading langgraph-0.2.50-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43 (from langgraph)\n",
      "  Downloading langchain_core-0.3.19-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting langgraph-checkpoint<3.0.0,>=2.0.4 (from langgraph)\n",
      "  Downloading langgraph_checkpoint-2.0.5-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting langgraph-sdk<0.2.0,>=0.1.32 (from langgraph)\n",
      "  Downloading langgraph_sdk-0.1.36-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (0.1.129)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (24.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (2.9.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (8.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (4.11.0)\n",
      "Collecting msgpack<2.0.0,>=1.1.0 (from langgraph-checkpoint<3.0.0,>=2.0.4->langgraph)\n",
      "  Downloading msgpack-1.1.0-cp312-cp312-macosx_10_9_x86_64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: httpx>=0.25.2 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from langgraph-sdk<0.2.0,>=0.1.32->langgraph) (0.27.2)\n",
      "Collecting httpx-sse>=0.4.0 (from langgraph-sdk<0.2.0,>=0.1.32->langgraph)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from langgraph-sdk<0.2.0,>=0.1.32->langgraph) (3.10.7)\n",
      "Requirement already satisfied: anyio in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.32->langgraph) (4.6.0)\n",
      "Requirement already satisfied: certifi in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.32->langgraph) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.32->langgraph) (1.0.5)\n",
      "Requirement already satisfied: idna in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.32->langgraph) (3.10)\n",
      "Requirement already satisfied: sniffio in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.32->langgraph) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.32->langgraph) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (3.0.0)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (2.32.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (2.2.3)\n",
      "Downloading langgraph-0.2.50-py3-none-any.whl (124 kB)\n",
      "Downloading langchain_core-0.3.19-py3-none-any.whl (409 kB)\n",
      "Downloading langgraph_checkpoint-2.0.5-py3-none-any.whl (24 kB)\n",
      "Downloading langgraph_sdk-0.1.36-py3-none-any.whl (29 kB)\n",
      "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading msgpack-1.1.0-cp312-cp312-macosx_10_9_x86_64.whl (85 kB)\n",
      "Installing collected packages: msgpack, httpx-sse, langgraph-sdk, langchain-core, langgraph-checkpoint, langgraph\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.3.6\n",
      "    Uninstalling langchain-core-0.3.6:\n",
      "      Successfully uninstalled langchain-core-0.3.6\n",
      "Successfully installed httpx-sse-0.4.0 langchain-core-0.3.19 langgraph-0.2.50 langgraph-checkpoint-2.0.5 langgraph-sdk-0.1.36 msgpack-1.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "sys.path.append(\"/Users/christiansarmiento/Library/CloudStorage/OneDrive-MaristCollege/Machine Learning/Private Code\")\n",
    "from api_keys import openAIKey\n",
    "from api_keys import langchainKey\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema import Document\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from typing import List, Dict, Optional\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import gradio as gr  # easy frontend implementation\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, SemanticSimilarity\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas import SingleTurnSample\n",
    "from ragas import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain Enviornment Variables\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = langchainKey()\n",
    "os.environ[\"OPENAI_API_KEY\"] = openAIKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "csvPath = \"/Users/christiansarmiento/Library/CloudStorage/OneDrive-MaristCollege/Machine Learning/Data/Marist_QA.csv\"\n",
    "maristQA = pd.read_csv(csvPath, header=None)\n",
    "\n",
    "# To use RecursiveCharacterTextSplitter, we need a list of dictionaries\n",
    "maristContext = [Document(page_content=text) for text in maristQA[1].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Documents into Chunks\n",
    "textSplitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, add_start_index=True)\n",
    "texts = textSplitter.split_documents(maristContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store Documents in Vector DB (Chroma)\n",
    "vectorDB = Chroma.from_documents(documents=texts, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# Setup Retrieval System\n",
    "retriever = vectorDB.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})  # Retrieves 3 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6g/8868nrgn5znbjrhdbycnn8pw0000gn/T/ipykernel_37269/630446541.py:30: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='no'\n"
     ]
    }
   ],
   "source": [
    "# Define Grader class for document grading in Self-RAG\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "# LLM with function call\n",
    "llmDocGrader = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "structuredLLMGrader = llmDocGrader.with_structured_output(GradeDocuments)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "   tic meaning related to the user question, grade it as relevant. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "gradePrompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Grader Chain\n",
    "retrievalGrader = gradePrompt | structuredLLMGrader\n",
    "\n",
    "# Testing it\n",
    "question = \"agent memory\"\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "docTxt = docs[1].page_content\n",
    "print(retrievalGrader.invoke({\"question\": question, \"document\": docTxt}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "# Generation Chain\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "ragChain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "generation = ragChain.invoke({\"context\": docs, \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradeHallucinations(binary_score='no')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define Hallucination Grader Class\n",
    "class GradeHallucinations(BaseModel):\n",
    "    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "# LLM with function call\n",
    "llmHallucinations = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "structuredLLMHallucinationGrader = llmHallucinations.with_structured_output(GradeHallucinations)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n \n",
    "     Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\"\n",
    "hallucinationPrompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "hallucinationGrader = hallucinationPrompt | structuredLLMHallucinationGrader\n",
    "hallucinationGrader.invoke({\"documents\": docs, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradeAnswer(binary_score='no')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Answer Grader\n",
    "class GradeAnswer(BaseModel):\n",
    "    \"\"\"Binary score to assess answer addresses question.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer addresses the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "# LLM with function call\n",
    "llmAnswerGrading = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "structuredLLMAnswerGrader = llmAnswerGrading.with_structured_output(GradeAnswer)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\n \n",
    "     Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question.\"\"\"\n",
    "answerPrompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Chain\n",
    "answerGrader = answerPrompt | structuredLLMAnswerGrader\n",
    "answerGrader.invoke({\"question\": question, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are the key concepts and functionalities of agent memory in artificial intelligence?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question Rewriter\n",
    "llmRewriter = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n \n",
    "     for vectorstore retrieval. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\n",
    "rewritePrompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "questionRewriter = rewritePrompt | llm | StrOutputParser()\n",
    "questionRewriter.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metric evaluator\n",
    "\n",
    "## Evaluation LLM & embeddings\n",
    "evalLLM = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\", temperature=0))\n",
    "evalEmbeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())\n",
    "\n",
    "## Initialize metrics with LLM and embeddings\n",
    "contextRecall = LLMContextRecall(llm=evalLLM)\n",
    "faithfulness = Faithfulness(llm=evalLLM)\n",
    "factualCorrectness = FactualCorrectness(llm=evalLLM)\n",
    "semanticSimilarity = SemanticSimilarity(embeddings=evalEmbeddings)\n",
    "\n",
    "## Collect metrics\n",
    "evalMetrics = [\n",
    "    contextRecall,\n",
    "    faithfulness,\n",
    "    factualCorrectness,\n",
    "    semanticSimilarity\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Graph structure for Self-RAG\n",
    "\n",
    "# Graph State\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        documents: list of documents\n",
    "        metrics: evaluation metrics for each generation \n",
    "    \"\"\"\n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: List[str]\n",
    "    metrics: Optional[Dict[str, float]]\n",
    "\n",
    "\n",
    "## Nodes\n",
    "\n",
    "# Retrieval Node     \n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.get_relevant_documents(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "# Generation Node\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # RAG generation\n",
    "    generation = ragChain.invoke({\"context\": documents, \"question\": question})\n",
    "\n",
    "    # Return updated state\n",
    "    updatedState = {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "    return updatedState\n",
    "\n",
    "# Grader Node\n",
    "def gradeDocuments(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with only filtered relevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "\n",
    "        score = retrievalGrader.invoke(\n",
    "            {\"question\": question, \"document\": d.page_content}\n",
    "        )\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            continue\n",
    "\n",
    "    return {\"documents\": filtered_docs, \"question\": question}\n",
    "\n",
    "\n",
    "# Rewriter node\n",
    "def transformQuery(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates question key with a re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Re-write question\n",
    "    better_question = questionRewriter.invoke({\"question\": question})\n",
    "    return {\"documents\": documents, \"question\": better_question}\n",
    "\n",
    "\n",
    "## Edges\n",
    "\n",
    "# Generation edge\n",
    "def decideToGenerate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or re-generate a question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    state[\"question\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if not filtered_documents:\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\n",
    "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\"\n",
    "        )\n",
    "        return \"transformQuery\"\n",
    "    \n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "\n",
    "\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    score = hallucinationGrader.invoke(\n",
    "        {\"documents\": documents, \"generation\": generation}\n",
    "    )\n",
    "    grade = score.binary_score\n",
    "\n",
    "    # Check hallucination\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "\n",
    "        # Check question-answering\n",
    "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "        score = answerGrader.invoke({\"question\": question, \"generation\": generation})\n",
    "        grade = score.binary_score\n",
    "\n",
    "        if grade == \"yes\":\n",
    "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "            return \"useful\"\n",
    "        \n",
    "        else:\n",
    "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "            return \"not useful\"\n",
    "        \n",
    "    else:\n",
    "        pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not supported\"\n",
    "\n",
    "async def evaluateMetrics(state):\n",
    "    \"\"\"\n",
    "    Evaluate metrics for the current RAG pipeline response.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state.\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Adds a 'metrics' key containing evaluation scores.\n",
    "    \"\"\"\n",
    "    print(\"---EVALUATING METRICS---\")\n",
    "    question = state[\"question\"]\n",
    "    generation = state[\"generation\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Mock ground truth if unavailable (replace with actual reference if possible)\n",
    "    groundTruth = state.get(\"groundTruth\", \"Expected answer based on context.\")\n",
    "\n",
    "    # Prepare retrieved contexts\n",
    "    retrievedContexts = [doc.page_content for doc in documents]\n",
    "\n",
    "    # Create a SingleTurnSample object\n",
    "    sample = SingleTurnSample(\n",
    "        user_input=question,\n",
    "        response=generation,\n",
    "        reference=groundTruth,\n",
    "        retrieved_contexts=retrievedContexts,\n",
    "    )\n",
    "\n",
    "    # Evaluate metrics\n",
    "    state[\"metrics\"] = {\n",
    "        \"LLMContextRecall\": await contextRecall.single_turn_ascore(sample),\n",
    "        \"Faithfulness\": await faithfulness.single_turn_ascore(sample),\n",
    "        \"FactualCorrectness\": await factualCorrectness.single_turn_ascore(sample),\n",
    "        \"SemanticSimilarity\": await semanticSimilarity.single_turn_ascore(sample),\n",
    "    }\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Self-RAG Graph\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"gradeDocuments\", gradeDocuments)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)  # generate\n",
    "workflow.add_node(\"transformQuery\", transformQuery)  # transform_query\n",
    "workflow.add_node(\"evaluateMetrics\", evaluateMetrics)\n",
    "\n",
    "# Build graph\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"gradeDocuments\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"gradeDocuments\",\n",
    "    decideToGenerate,\n",
    "    {\n",
    "        \"transformQuery\": \"transformQuery\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"transformQuery\", \"retrieve\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        \"not supported\": \"generate\",\n",
    "        \"useful\": \"evaluateMetrics\",\n",
    "        \"not useful\": \"transformQuery\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"generate\", \"evaluateMetrics\")\n",
    "workflow.add_edge(\"evaluateMetrics\", END)\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "\"Node 'retrieve':\"\n",
      "'\\n---\\n'\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "\"Node 'gradeDocuments':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "---EVALUATING METRICS---\n",
      "\"Node 'evaluateMetrics':\"\n",
      "'Metrics: '\n",
      "{'FactualCorrectness': 0.0,\n",
      " 'Faithfulness': 1.0,\n",
      " 'LLMContextRecall': 0.0,\n",
      " 'SemanticSimilarity': 0.7259663840810067}\n",
      "'\\n---\\n'\n",
      "'Final Generation: '\n",
      "('Dr. Carolyn C. Matheus is an Associate Professor of Information Systems and '\n",
      " 'the Director of the Honors Program at Marist College. She holds a PhD in '\n",
      " 'Organizational Studies with a focus on leadership from SUNY Albany and has '\n",
      " 'received the National Society of Leadership and Success award for Excellence '\n",
      " 'in Teaching. Dr. Matheus is involved in faculty-student research projects '\n",
      " 'and offers seminars on authentic leadership and innovation.')\n",
      "'Final Metrics: '\n",
      "{'FactualCorrectness': 0.0,\n",
      " 'Faithfulness': 1.0,\n",
      " 'LLMContextRecall': 0.0,\n",
      " 'SemanticSimilarity': 0.7259663840810067}\n"
     ]
    }
   ],
   "source": [
    "# Inital test run\n",
    "inputs = {\"question\": \"Who is Carolyn Matheus?\"}\n",
    "async for output in app.astream(inputs):\n",
    "    for key, value in output.items():\n",
    "\n",
    "        # Print node\n",
    "        pprint(f\"Node '{key}':\")\n",
    "\n",
    "        # Optional: print full state at each node\n",
    "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "\n",
    "        # Print metrics\n",
    "        if \"metrics\" in value:\n",
    "            pprint(\"Metrics: \")\n",
    "            pprint(value[\"metrics\"])\n",
    "\n",
    "    pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint(\"Final Generation: \")\n",
    "pprint(value[\"generation\"])\n",
    "\n",
    "# Final metrics\n",
    "if \"metrics\" in value:\n",
    "    pprint(\"Final Metrics: \")\n",
    "    pprint(value[\"metrics\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put SelfRAG into Gradio\n",
    "evaluationSamples = []\n",
    "async def selfRAG(userQuery, history, correctAnswer=None):\n",
    "    \"\"\"\n",
    "    Gradio-compatible function to process SelfRAG workflow.\n",
    "    Args:\n",
    "        userQuery (str): The user's question.\n",
    "        history (list): Conversation history.\n",
    "        correctAnswer (str): The ground truth answer for metrics (optional).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (chatDisplay, history)\n",
    "    \"\"\"\n",
    "\n",
    "    # Get our input together\n",
    "    inputs = {\"question\": userQuery}\n",
    "\n",
    "    # Start the workflow\n",
    "    finalOutput = None\n",
    "    async for output in app.astream(inputs):\n",
    "\n",
    "        # Saving final output for metric purposes\n",
    "        finalOutput = output\n",
    "\n",
    "        # Printing out each node state for clarity\n",
    "        for key, value in output.items():\n",
    "\n",
    "            # Print node\n",
    "            pprint(f\"Node '{key}':\")\n",
    "\n",
    "            # Print metrics\n",
    "            if \"metrics\" in value:\n",
    "                pprint(\"Metrics: \")\n",
    "                pprint(value[\"metrics\"])\n",
    "    \n",
    "    # Get the generation and its metrics\n",
    "    finalNodeKey = list(finalOutput.keys())[-1]  # Get the key of the last executed node\n",
    "    nodeOutput = finalOutput[finalNodeKey]  # Access the nested state\n",
    "    generation = nodeOutput.get(\"generation\", \"No generation produced.\")\n",
    "    metrics = nodeOutput.get(\"metrics\", {})\n",
    "\n",
    "    # Update history\n",
    "    if history is None:\n",
    "        history = []\n",
    "    \n",
    "    history.extend([\n",
    "        {\"role\": \"user\", \"content\": userQuery},\n",
    "        {\"role\": \"llm\", \"content\": generation}\n",
    "    ])\n",
    "\n",
    "    # Display output for gradio\n",
    "    chatDisplay = [(msg[\"content\"], \"User\" if msg[\"role\"] == \"user\" else \"LLM\") for msg in history]\n",
    "\n",
    "    # Append metrics to evaluationSamples for tracking (if correctAnswer is provided)\n",
    "    if correctAnswer:\n",
    "        evaluationSamples.append({\n",
    "            \"user_input\": userQuery,\n",
    "            \"retrieved_contexts\": [doc.page_content for doc in finalOutput.get(\"documents\", [])],\n",
    "            \"response\": generation,\n",
    "            \"reference\": correctAnswer,\n",
    "            \"metrics\": metrics,\n",
    "        })\n",
    "\n",
    "    return chatDisplay, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christiansarmiento/opt/anaconda3/envs/LLM-LangChain/lib/python3.12/site-packages/gradio/components/chatbot.py:222: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7871\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7871/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "\"Node 'retrieve':\"\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "\"Node 'gradeDocuments':\"\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "\"Node 'generate':\"\n",
      "---EVALUATING METRICS---\n",
      "\"Node 'evaluateMetrics':\"\n",
      "'Metrics: '\n",
      "{'FactualCorrectness': 0.0,\n",
      " 'Faithfulness': 1.0,\n",
      " 'LLMContextRecall': 0.0,\n",
      " 'SemanticSimilarity': 0.7259663840810067}\n",
      "---RETRIEVE---\n",
      "\"Node 'retrieve':\"\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "\"Node 'gradeDocuments':\"\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "\"Node 'generate':\"\n",
      "---EVALUATING METRICS---\n",
      "\"Node 'evaluateMetrics':\"\n",
      "'Metrics: '\n",
      "{'FactualCorrectness': 0.0,\n",
      " 'Faithfulness': 1.0,\n",
      " 'LLMContextRecall': 0.0,\n",
      " 'SemanticSimilarity': 0.7259663840810067}\n"
     ]
    }
   ],
   "source": [
    "# Gradio frontend\n",
    "interface = gr.Interface(\n",
    "    fn=selfRAG,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Ask a Question\", placeholder=\"Enter your question here...\"),\n",
    "        gr.State(),  # Keeps track of conversation history\n",
    "        gr.Textbox(label=\"Correct Answer (Optional)\", placeholder=\"For evaluation purposes...\"),\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Chatbot(label=\"SelfRAG Conversation\"),\n",
    "        gr.State(),  # Updates conversation history\n",
    "    ],\n",
    "    title=\"Self-RAG Implementation\",\n",
    "    description=\"Interact with the Self-RAG workflow for document-grounded question answering.\",\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate our RAG pipeline when given ground truth\n",
    "async def pipelineEvaluation(dataset, metrics):\n",
    "\n",
    "    # Run through our runs\n",
    "    results = []\n",
    "    for run in dataset:\n",
    "\n",
    "        # Save our inputs/outputs\n",
    "        inputQuery = run[\"user_input\"]\n",
    "        groundTruthAnswer = run[\"reference\"]\n",
    "        contexts = run[\"retrieved_contexts\"]\n",
    "        response = run[\"response\"]\n",
    "\n",
    "        # Create a SingleTurnSample object\n",
    "        sample = SingleTurnSample(\n",
    "            user_input=inputQuery,\n",
    "            response=response,\n",
    "            reference=groundTruthAnswer,\n",
    "            retrieved_contexts=contexts \n",
    "        )\n",
    "\n",
    "        # Evaluate metrics\n",
    "        runResults = {\"input_query\": inputQuery}\n",
    "        for metric in metrics:\n",
    "\n",
    "            # Get the score for the given metric\n",
    "            try:\n",
    "\n",
    "                score = await metric.single_turn_ascore(sample)\n",
    "                runResults[type(metric).__name__] = score\n",
    "\n",
    "            except Exception as e:\n",
    "                # Catch errors for debugging\n",
    "                runResults[type(metric).__name__] = f\"Error: {str(e)}\"\n",
    "        \n",
    "        # Save metric results\n",
    "        results.append(runResults)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metrics\n",
    "evalMetrics = [LLMContextRecall(llm=LangchainLLMWrapper(llm)), \n",
    "               FactualCorrectness(llm=LangchainLLMWrapper(llm)), \n",
    "               Faithfulness(llm=LangchainLLMWrapper(llm)), \n",
    "               SemanticSimilarity(embeddings=LangchainEmbeddingsWrapper(OpenAIEmbeddings()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_query': 'Who is Carolyn Matheus?', 'LLMContextRecall': 0.0, 'FactualCorrectness': 0.0, 'Faithfulness': 0.0, 'SemanticSimilarity': 0.928909234587568}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate our pipeline responses\n",
    "evalResults = await pipelineEvaluation(evaluationSamples, evalMetrics)\n",
    "for result in evalResults:\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM-LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
